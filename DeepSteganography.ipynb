{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessary libraries and modules\n",
    "from itertools import islice\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import os \n",
    "import pickle\n",
    "from torchvision import datasets, utils\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from random import shuffle\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamza_ELF\\anaconda3\\envs\\AI_labs\\DeepSteg\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "# os.chdir(\"..\")\n",
    "cwd = os.getcwd()+'/input'\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 3\n",
    "batch_size = 2\n",
    "learning_rate = 0.0001\n",
    "beta = 1\n",
    "\n",
    "# Mean and std deviation of imagenet dataset. Source: http://cs231n.stanford.edu/reports/2017/pdfs/101.pdf\n",
    "std = [0.229, 0.224, 0.225]\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "# TODO: Define train, validation and models\n",
    "MODELS_PATH = os.getcwd()+'/output/models/'\n",
    "# TRAIN_PATH = cwd+'/train/'\n",
    "# VALID_PATH = cwd+'/valid/'\n",
    "VALID_PATH = cwd+'/sample/valid/'\n",
    "TRAIN_PATH = cwd+'/sample/train/'\n",
    "TEST_PATH = cwd+'/test/'\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "if not os.path.exists(MODELS_PATH): os.mkdir(MODELS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of useful functions we are going to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_loss(S_prime, C_prime, S, C, B):\n",
    "    ''' Calculates loss specified on the paper.'''\n",
    "    \n",
    "    loss_cover = torch.nn.functional.mse_loss(C_prime, C)\n",
    "    loss_secret = torch.nn.functional.mse_loss(S_prime, S)\n",
    "    loss_all = loss_cover + B * loss_secret\n",
    "    return loss_all, loss_cover, loss_secret\n",
    "\n",
    "def denormalize(image, std, mean):\n",
    "    ''' Denormalizes a tensor of images.'''\n",
    "\n",
    "    for t in range(3):\n",
    "        image[t, :, :] = (image[t, :, :] * std[t]) + mean[t]\n",
    "    return image\n",
    "\n",
    "def imshow(img, idx, learning_rate, beta):\n",
    "    '''Prints out an image given in tensor format.'''\n",
    "    \n",
    "    img = denormalize(img, std, mean)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title('Example '+str(idx)+', lr='+str(learning_rate)+', B='+str(beta))\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def gaussian(tensor, mean=0, stddev=0.1):\n",
    "    '''Adds random noise to a tensor.'''\n",
    "    \n",
    "    noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1)\n",
    "    return Variable(tensor + noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author used 3x3, 4x4 and 5x5 each for the first four layers and the concatenated them and passed them to the final layer. \n",
    "\n",
    "This architecture was replicated exactly in each of the layers. Output of PrepNetwork (secret image preapred to merge with cover) is concatenated to the cover and fed into the hidding network. The hidding network hides the secret image in the PrepNetwork's output and returns the hidden image. This is fed into the Reveal network to output the message, that should be close to the secret image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation Network (2 conv layers)\n",
    "class PrepNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrepNetwork, self).__init__()\n",
    "        self.initialP3 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialP4 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialP5 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalP3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalP4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalP5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, p):\n",
    "        p1 = self.initialP3(p)\n",
    "        p2 = self.initialP4(p)\n",
    "        p3 = self.initialP5(p)\n",
    "        mid = torch.cat((p1, p2, p3), 1)\n",
    "        p4 = self.finalP3(mid)\n",
    "        p5 = self.finalP4(mid)\n",
    "        p6 = self.finalP5(mid)\n",
    "        out = torch.cat((p4, p5, p6), 1)\n",
    "        return out\n",
    "\n",
    "# Hiding Network (5 conv layers)\n",
    "class HidingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HidingNetwork, self).__init__()\n",
    "        self.initialH3 = nn.Sequential(\n",
    "            nn.Conv2d(153, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialH4 = nn.Sequential(\n",
    "            nn.Conv2d(153, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialH5 = nn.Sequential(\n",
    "            nn.Conv2d(153, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalH4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH = nn.Sequential(\n",
    "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
    "        \n",
    "    def forward(self, h):\n",
    "        h1 = self.initialH3(h)\n",
    "        h2 = self.initialH4(h)\n",
    "        h3 = self.initialH5(h)\n",
    "        mid = torch.cat((h1, h2, h3), 1)\n",
    "        h4 = self.finalH3(mid)\n",
    "        h5 = self.finalH4(mid)\n",
    "        h6 = self.finalH5(mid)\n",
    "        mid2 = torch.cat((h4, h5, h6), 1)\n",
    "        out = self.finalH(mid2)\n",
    "        out_noise = gaussian(out.data, 0, 0.1)\n",
    "        return out, out_noise\n",
    "\n",
    "# Reveal Network (2 conv layers)\n",
    "class RevealNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RevealNetwork, self).__init__()\n",
    "        self.initialR3 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialR4 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialR5 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalR4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR = nn.Sequential(\n",
    "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, r):\n",
    "        r1 = self.initialR3(r)\n",
    "        r2 = self.initialR4(r)\n",
    "        r3 = self.initialR5(r)\n",
    "        mid = torch.cat((r1, r2, r3), 1)\n",
    "        r4 = self.finalR3(mid)\n",
    "        r5 = self.finalR4(mid)\n",
    "        r6 = self.finalR5(mid)\n",
    "        mid2 = torch.cat((r4, r5, r6), 1)\n",
    "        out = self.finalR(mid2)\n",
    "        return out\n",
    "\n",
    "# Join three networks in one module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.m1 = PrepNetwork()\n",
    "        self.m2 = HidingNetwork()\n",
    "        self.m3 = RevealNetwork()\n",
    "\n",
    "    def forward(self, secret, cover):\n",
    "        x_1 = self.m1(secret)\n",
    "        mid = torch.cat((x_1, cover), 1)\n",
    "        x_2, x_2_noise = self.m2(mid)\n",
    "        x_3 = self.m3(x_2_noise)\n",
    "        return x_2, x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates net object\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create loaders for normalized training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamza_ELF\\anaconda3\\envs\\AI_labs\\lib\\site-packages\\torchvision\\transforms\\transforms.py:279: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "# Creates training set\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(\n",
    "        TRAIN_PATH,\n",
    "        transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean,\n",
    "        std=std)\n",
    "        ])), batch_size=batch_size, num_workers=1, \n",
    "        pin_memory=True, shuffle=True, drop_last=True)\n",
    "\n",
    "# Creates test set\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(\n",
    "        TEST_PATH, \n",
    "        transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean,\n",
    "        std=std)\n",
    "        ])), batch_size=2, num_workers=1, \n",
    "        pin_memory=True, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model and validate it, saving the best model. We use Adam as an optimizer as the paper specifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, beta, learning_rate):\n",
    "    \n",
    "    # Save optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_history = []\n",
    "    # Iterate over batches performing forward and backward passes\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Train mode\n",
    "        net.train()\n",
    "        \n",
    "        train_losses = []\n",
    "        # Train one epoch\n",
    "        for idx, train_batch in enumerate(train_loader):\n",
    "\n",
    "            data, _  = train_batch\n",
    "\n",
    "            # Saves secret images and secret covers\n",
    "            train_covers = data[:len(data)//2]\n",
    "            train_secrets = data[len(data)//2:]\n",
    "            \n",
    "            # Creates variable from secret and cover images\n",
    "            train_secrets = Variable(train_secrets, requires_grad=False)\n",
    "            train_covers = Variable(train_covers, requires_grad=False)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            train_hidden, train_output = net(train_secrets, train_covers)\n",
    "\n",
    "            # Calculate loss and perform backprop\n",
    "            train_loss, train_loss_cover, train_loss_secret = customized_loss(train_output, train_hidden, train_secrets, train_covers, beta)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            # Saves training loss\n",
    "            train_losses.append(train_loss.data)\n",
    "            loss_history.append(train_loss.data)\n",
    "            \n",
    "            # Prints mini-batch losses\n",
    "            print('Training: Batch {0}/{1}. Loss of {2:.4f}, cover loss of {3:.4f}, secret loss of {4:.4f}'.format(idx+1, len(train_loader), train_loss.data, train_loss_cover.data, train_loss_secret.data))\n",
    "    \n",
    "        torch.save(net.state_dict(), MODELS_PATH+'Epoch N{}.pkl'.format(epoch+1))\n",
    "        \n",
    "        mean_train_loss = np.mean(train_losses)\n",
    "    \n",
    "        # Prints epoch average loss\n",
    "        print ('Epoch [{0}/{1}], Average_loss: {2:.4f}'.format(\n",
    "                epoch+1, num_epochs, mean_train_loss))\n",
    "    \n",
    "    return net, mean_train_loss, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamza_ELF\\anaconda3\\envs\\AI_labs\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "<ipython-input-3-723a443b100a>:29: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch 1/50000. Loss of 3.2631, cover loss of 1.2426, secret loss of 2.0205\n",
      "Training: Batch 2/50000. Loss of 3.3719, cover loss of 0.9517, secret loss of 2.4202\n",
      "Training: Batch 3/50000. Loss of 2.0098, cover loss of 1.3066, secret loss of 0.7032\n",
      "Training: Batch 4/50000. Loss of 1.0303, cover loss of 0.5804, secret loss of 0.4500\n",
      "Training: Batch 5/50000. Loss of 2.6844, cover loss of 1.3938, secret loss of 1.2907\n",
      "Training: Batch 6/50000. Loss of 2.7686, cover loss of 1.7255, secret loss of 1.0431\n",
      "Training: Batch 7/50000. Loss of 2.2211, cover loss of 0.9372, secret loss of 1.2839\n",
      "Training: Batch 8/50000. Loss of 2.6488, cover loss of 1.8937, secret loss of 0.7551\n",
      "Training: Batch 9/50000. Loss of 1.9033, cover loss of 0.0657, secret loss of 1.8376\n",
      "Training: Batch 10/50000. Loss of 2.5948, cover loss of 1.4906, secret loss of 1.1042\n",
      "Training: Batch 11/50000. Loss of 4.8929, cover loss of 3.2283, secret loss of 1.6646\n",
      "Training: Batch 12/50000. Loss of 3.3360, cover loss of 2.3042, secret loss of 1.0318\n",
      "Training: Batch 13/50000. Loss of 1.8870, cover loss of 0.7941, secret loss of 1.0929\n",
      "Training: Batch 14/50000. Loss of 5.2360, cover loss of 1.5400, secret loss of 3.6960\n",
      "Training: Batch 15/50000. Loss of 3.5814, cover loss of 1.7586, secret loss of 1.8228\n",
      "Training: Batch 16/50000. Loss of 3.0317, cover loss of 1.4000, secret loss of 1.6318\n",
      "Training: Batch 17/50000. Loss of 1.9007, cover loss of 1.1914, secret loss of 0.7093\n",
      "Training: Batch 18/50000. Loss of 3.1284, cover loss of 0.2828, secret loss of 2.8456\n",
      "Training: Batch 19/50000. Loss of 1.7250, cover loss of 0.6083, secret loss of 1.1167\n",
      "Training: Batch 20/50000. Loss of 1.8955, cover loss of 0.5714, secret loss of 1.3241\n",
      "Training: Batch 21/50000. Loss of 2.1712, cover loss of 1.1380, secret loss of 1.0332\n",
      "Training: Batch 22/50000. Loss of 2.0342, cover loss of 1.4155, secret loss of 0.6187\n",
      "Training: Batch 23/50000. Loss of 1.6150, cover loss of 0.6214, secret loss of 0.9935\n",
      "Training: Batch 24/50000. Loss of 1.6928, cover loss of 0.4344, secret loss of 1.2584\n",
      "Training: Batch 25/50000. Loss of 0.9788, cover loss of 0.1967, secret loss of 0.7821\n",
      "Training: Batch 26/50000. Loss of 2.1282, cover loss of 1.0918, secret loss of 1.0364\n",
      "Training: Batch 27/50000. Loss of 1.7085, cover loss of 0.6212, secret loss of 1.0873\n",
      "Training: Batch 28/50000. Loss of 2.4479, cover loss of 1.1310, secret loss of 1.3170\n",
      "Training: Batch 29/50000. Loss of 1.4369, cover loss of 0.4029, secret loss of 1.0339\n",
      "Training: Batch 30/50000. Loss of 2.0905, cover loss of 0.3356, secret loss of 1.7549\n",
      "Training: Batch 31/50000. Loss of 1.4035, cover loss of 0.3940, secret loss of 1.0095\n",
      "Training: Batch 32/50000. Loss of 1.8323, cover loss of 0.2302, secret loss of 1.6021\n",
      "Training: Batch 33/50000. Loss of 2.1094, cover loss of 0.5188, secret loss of 1.5907\n",
      "Training: Batch 34/50000. Loss of 2.5140, cover loss of 0.3710, secret loss of 2.1431\n",
      "Training: Batch 35/50000. Loss of 1.9033, cover loss of 0.2786, secret loss of 1.6248\n",
      "Training: Batch 36/50000. Loss of 2.9294, cover loss of 1.6404, secret loss of 1.2890\n",
      "Training: Batch 37/50000. Loss of 1.0794, cover loss of 0.2960, secret loss of 0.7834\n",
      "Training: Batch 38/50000. Loss of 1.0049, cover loss of 0.2748, secret loss of 0.7301\n",
      "Training: Batch 39/50000. Loss of 1.3348, cover loss of 0.3270, secret loss of 1.0077\n",
      "Training: Batch 40/50000. Loss of 0.9609, cover loss of 0.1999, secret loss of 0.7610\n",
      "Training: Batch 41/50000. Loss of 2.1475, cover loss of 0.1912, secret loss of 1.9563\n",
      "Training: Batch 42/50000. Loss of 1.6116, cover loss of 0.4873, secret loss of 1.1243\n",
      "Training: Batch 43/50000. Loss of 1.2161, cover loss of 0.2590, secret loss of 0.9571\n",
      "Training: Batch 44/50000. Loss of 0.7312, cover loss of 0.1520, secret loss of 0.5792\n",
      "Training: Batch 45/50000. Loss of 1.4766, cover loss of 0.3094, secret loss of 1.1672\n",
      "Training: Batch 46/50000. Loss of 1.6557, cover loss of 0.1006, secret loss of 1.5551\n",
      "Training: Batch 47/50000. Loss of 1.6612, cover loss of 0.2287, secret loss of 1.4326\n",
      "Training: Batch 48/50000. Loss of 3.6010, cover loss of 1.0303, secret loss of 2.5707\n",
      "Training: Batch 49/50000. Loss of 1.0132, cover loss of 0.2092, secret loss of 0.8040\n",
      "Training: Batch 50/50000. Loss of 2.3587, cover loss of 0.2399, secret loss of 2.1188\n",
      "Training: Batch 51/50000. Loss of 0.8629, cover loss of 0.0998, secret loss of 0.7630\n",
      "Training: Batch 52/50000. Loss of 2.3881, cover loss of 0.4348, secret loss of 1.9533\n",
      "Training: Batch 53/50000. Loss of 1.7047, cover loss of 0.2181, secret loss of 1.4866\n",
      "Training: Batch 54/50000. Loss of 1.6417, cover loss of 0.1293, secret loss of 1.5124\n",
      "Training: Batch 55/50000. Loss of 1.2419, cover loss of 0.2690, secret loss of 0.9729\n",
      "Training: Batch 56/50000. Loss of 1.5516, cover loss of 0.4309, secret loss of 1.1207\n",
      "Training: Batch 57/50000. Loss of 1.3615, cover loss of 0.2128, secret loss of 1.1487\n",
      "Training: Batch 58/50000. Loss of 2.7984, cover loss of 0.3353, secret loss of 2.4631\n",
      "Training: Batch 59/50000. Loss of 0.8443, cover loss of 0.4010, secret loss of 0.4433\n",
      "Training: Batch 60/50000. Loss of 1.1608, cover loss of 0.5702, secret loss of 0.5905\n",
      "Training: Batch 61/50000. Loss of 1.6915, cover loss of 0.1805, secret loss of 1.5109\n",
      "Training: Batch 62/50000. Loss of 1.1335, cover loss of 0.1695, secret loss of 0.9640\n",
      "Training: Batch 63/50000. Loss of 1.7382, cover loss of 0.5318, secret loss of 1.2064\n",
      "Training: Batch 64/50000. Loss of 1.7877, cover loss of 0.2003, secret loss of 1.5874\n",
      "Training: Batch 65/50000. Loss of 2.0098, cover loss of 0.4840, secret loss of 1.5258\n",
      "Training: Batch 66/50000. Loss of 1.6458, cover loss of 0.1553, secret loss of 1.4905\n",
      "Training: Batch 67/50000. Loss of 0.6707, cover loss of 0.1795, secret loss of 0.4912\n",
      "Training: Batch 68/50000. Loss of 2.9245, cover loss of 0.8743, secret loss of 2.0502\n",
      "Training: Batch 69/50000. Loss of 3.0153, cover loss of 0.2069, secret loss of 2.8085\n",
      "Training: Batch 70/50000. Loss of 1.4528, cover loss of 0.2340, secret loss of 1.2188\n",
      "Training: Batch 71/50000. Loss of 1.0772, cover loss of 0.1925, secret loss of 0.8847\n",
      "Training: Batch 72/50000. Loss of 1.6605, cover loss of 0.3113, secret loss of 1.3492\n",
      "Training: Batch 73/50000. Loss of 0.5268, cover loss of 0.2195, secret loss of 0.3073\n",
      "Training: Batch 74/50000. Loss of 1.1017, cover loss of 0.3215, secret loss of 0.7802\n",
      "Training: Batch 75/50000. Loss of 1.2001, cover loss of 0.2086, secret loss of 0.9915\n",
      "Training: Batch 76/50000. Loss of 0.5226, cover loss of 0.0818, secret loss of 0.4408\n",
      "Training: Batch 77/50000. Loss of 0.9424, cover loss of 0.1671, secret loss of 0.7753\n",
      "Training: Batch 78/50000. Loss of 1.3514, cover loss of 0.2871, secret loss of 1.0643\n",
      "Training: Batch 79/50000. Loss of 1.6001, cover loss of 0.2418, secret loss of 1.3583\n",
      "Training: Batch 80/50000. Loss of 1.9960, cover loss of 0.1079, secret loss of 1.8881\n",
      "Training: Batch 81/50000. Loss of 1.9101, cover loss of 0.1740, secret loss of 1.7361\n",
      "Training: Batch 82/50000. Loss of 1.8511, cover loss of 0.4151, secret loss of 1.4360\n",
      "Training: Batch 83/50000. Loss of 1.8914, cover loss of 0.4224, secret loss of 1.4690\n",
      "Training: Batch 84/50000. Loss of 1.0264, cover loss of 0.2413, secret loss of 0.7851\n",
      "Training: Batch 85/50000. Loss of 0.6214, cover loss of 0.2285, secret loss of 0.3929\n",
      "Training: Batch 86/50000. Loss of 2.6902, cover loss of 0.2365, secret loss of 2.4537\n",
      "Training: Batch 87/50000. Loss of 3.7203, cover loss of 0.2945, secret loss of 3.4259\n",
      "Training: Batch 88/50000. Loss of 0.6911, cover loss of 0.0886, secret loss of 0.6025\n",
      "Training: Batch 89/50000. Loss of 1.1312, cover loss of 0.1576, secret loss of 0.9736\n",
      "Training: Batch 90/50000. Loss of 1.2081, cover loss of 0.1612, secret loss of 1.0469\n",
      "Training: Batch 91/50000. Loss of 1.0166, cover loss of 0.1491, secret loss of 0.8675\n",
      "Training: Batch 92/50000. Loss of 0.7709, cover loss of 0.1384, secret loss of 0.6325\n",
      "Training: Batch 93/50000. Loss of 1.3454, cover loss of 0.1333, secret loss of 1.2121\n",
      "Training: Batch 94/50000. Loss of 1.4528, cover loss of 0.5211, secret loss of 0.9317\n",
      "Training: Batch 95/50000. Loss of 2.0645, cover loss of 0.5024, secret loss of 1.5621\n",
      "Training: Batch 96/50000. Loss of 1.6886, cover loss of 0.1729, secret loss of 1.5157\n",
      "Training: Batch 97/50000. Loss of 2.0542, cover loss of 0.2455, secret loss of 1.8087\n",
      "Training: Batch 98/50000. Loss of 1.9532, cover loss of 0.1682, secret loss of 1.7850\n",
      "Training: Batch 99/50000. Loss of 1.9685, cover loss of 0.7107, secret loss of 1.2578\n",
      "Training: Batch 100/50000. Loss of 1.6324, cover loss of 0.1437, secret loss of 1.4888\n",
      "Training: Batch 101/50000. Loss of 2.9012, cover loss of 0.1519, secret loss of 2.7492\n",
      "Training: Batch 102/50000. Loss of 1.2877, cover loss of 0.3187, secret loss of 0.9690\n",
      "Training: Batch 103/50000. Loss of 3.7779, cover loss of 0.2475, secret loss of 3.5304\n",
      "Training: Batch 104/50000. Loss of 1.5464, cover loss of 0.2130, secret loss of 1.3334\n",
      "Training: Batch 105/50000. Loss of 2.7603, cover loss of 1.3574, secret loss of 1.4028\n",
      "Training: Batch 106/50000. Loss of 1.9679, cover loss of 0.2293, secret loss of 1.7386\n",
      "Training: Batch 107/50000. Loss of 1.9592, cover loss of 0.1338, secret loss of 1.8254\n",
      "Training: Batch 108/50000. Loss of 2.1604, cover loss of 0.5080, secret loss of 1.6524\n",
      "Training: Batch 109/50000. Loss of 1.8976, cover loss of 0.2552, secret loss of 1.6425\n",
      "Training: Batch 110/50000. Loss of 2.2781, cover loss of 0.4933, secret loss of 1.7848\n",
      "Training: Batch 111/50000. Loss of 1.4206, cover loss of 0.1247, secret loss of 1.2958\n",
      "Training: Batch 112/50000. Loss of 0.8015, cover loss of 0.0441, secret loss of 0.7574\n",
      "Training: Batch 113/50000. Loss of 0.7996, cover loss of 0.1305, secret loss of 0.6692\n",
      "Training: Batch 114/50000. Loss of 1.5858, cover loss of 0.0593, secret loss of 1.5265\n",
      "Training: Batch 115/50000. Loss of 1.0339, cover loss of 0.1255, secret loss of 0.9083\n",
      "Training: Batch 116/50000. Loss of 1.8357, cover loss of 0.0793, secret loss of 1.7564\n",
      "Training: Batch 117/50000. Loss of 1.3694, cover loss of 0.2631, secret loss of 1.1063\n"
     ]
    }
   ],
   "source": [
    "net, mean_train_loss, loss_history = train_model(train_loader, beta, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss through epochs\n",
    "plt.plot(loss_history)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the model and print out a few images so we can visually see how good a job the model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.load_state_dict(torch.load(MODELS_PATH+'Epoch N4.pkl'))\n",
    "\n",
    "# Switch to evaluate mode\n",
    "net.eval()\n",
    "\n",
    "test_losses = []\n",
    "# Show images\n",
    "for idx, test_batch in enumerate(test_loader):\n",
    "     # Saves images\n",
    "    data, _ = test_batch\n",
    "\n",
    "    # Saves secret images and secret covers\n",
    "    test_secret = data[:len(data)//2]\n",
    "    test_cover = data[len(data)//2:]\n",
    "\n",
    "    # Creates variable from secret and cover images\n",
    "    test_secret = Variable(test_secret, volatile=True)\n",
    "    test_cover = Variable(test_cover, volatile=True)\n",
    "\n",
    "    # Compute output\n",
    "    test_hidden, test_output = net(test_secret, test_cover)\n",
    "    \n",
    "    # Calculate loss\n",
    "    test_loss, loss_cover, loss_secret = customized_loss(test_output, test_hidden, test_secret, test_cover, beta)\n",
    "    \n",
    "#     diff_S, diff_C = np.abs(np.array(test_output.data[0]) - np.array(test_secret.data[0])), np.abs(np.array(test_hidden.data[0]) - np.array(test_cover.data[0]))\n",
    "    \n",
    "#     print (diff_S, diff_C)\n",
    "    \n",
    "    if idx in [1,2,3,4]:\n",
    "        print ('Total loss: {:.2f} \\nLoss on secret: {:.2f} \\nLoss on cover: {:.2f}'.format(test_loss.data, loss_secret.data, loss_cover.data))\n",
    "\n",
    "        # Creates img tensor\n",
    "        imgs = [test_secret.data, test_output.data, test_cover.data, test_hidden.data]\n",
    "        imgs_tsor = torch.cat(imgs, 0)\n",
    "\n",
    "        # Prints Images\n",
    "        imshow(utils.make_grid(imgs_tsor), idx+1, learning_rate=learning_rate, beta=beta)\n",
    "        \n",
    "    test_losses.append(test_loss.data)\n",
    "        \n",
    "mean_test_loss = np.mean(test_losses)\n",
    "\n",
    "print ('Average loss on test set: {:.2f}'.format(mean_test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
