{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessary libraries and modules\n",
    "from itertools import islice\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import os \n",
    "import pickle\n",
    "from torchvision import datasets, utils\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from random import shuffle\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamza_ELF\\anaconda3\\envs\\AI_labs\\DeepSteg\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "# os.chdir(\"..\")\n",
    "cwd = os.getcwd()+'/input'\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 3\n",
    "batch_size = 2\n",
    "learning_rate = 0.0001\n",
    "beta = 1\n",
    "\n",
    "# Mean and std deviation of imagenet dataset. Source: http://cs231n.stanford.edu/reports/2017/pdfs/101.pdf\n",
    "std = [0.229, 0.224, 0.225]\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "# TODO: Define train, validation and models\n",
    "MODELS_PATH = os.getcwd()+'/output/models/'\n",
    "# TRAIN_PATH = cwd+'/train/'\n",
    "# VALID_PATH = cwd+'/valid/'\n",
    "VALID_PATH = cwd+'/sample/valid/'\n",
    "TRAIN_PATH = cwd+'/sample/train/'\n",
    "TEST_PATH = cwd+'/test/'\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "if not os.path.exists(MODELS_PATH): os.mkdir(MODELS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of useful functions we are going to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_loss(S_prime, C_prime, S, C, B):\n",
    "    ''' Calculates loss specified on the paper.'''\n",
    "    \n",
    "    loss_cover = torch.nn.functional.mse_loss(C_prime, C)\n",
    "    loss_secret = torch.nn.functional.mse_loss(S_prime, S)\n",
    "    loss_all = loss_cover + B * loss_secret\n",
    "    return loss_all, loss_cover, loss_secret\n",
    "\n",
    "def denormalize(image, std, mean):\n",
    "    ''' Denormalizes a tensor of images.'''\n",
    "\n",
    "    for t in range(3):\n",
    "        image[t, :, :] = (image[t, :, :] * std[t]) + mean[t]\n",
    "    return image\n",
    "\n",
    "def imshow(img, idx, learning_rate, beta):\n",
    "    '''Prints out an image given in tensor format.'''\n",
    "    \n",
    "    img = denormalize(img, std, mean)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title('Example '+str(idx)+', lr='+str(learning_rate)+', B='+str(beta))\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def gaussian(tensor, mean=0, stddev=0.1):\n",
    "    '''Adds random noise to a tensor.'''\n",
    "    \n",
    "    noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1)\n",
    "    return Variable(tensor + noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author used 3x3, 4x4 and 5x5 each for the first four layers and the concatenated them and passed them to the final layer. \n",
    "\n",
    "This architecture was replicated exactly in each of the layers. Output of PrepNetwork (secret image preapred to merge with cover) is concatenated to the cover and fed into the hidding network. The hidding network hides the secret image in the PrepNetwork's output and returns the hidden image. This is fed into the Reveal network to output the message, that should be close to the secret image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation Network (2 conv layers)\n",
    "class PrepNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrepNetwork, self).__init__()\n",
    "        self.initialP3 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialP4 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialP5 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalP3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalP4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalP5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, p):\n",
    "        p1 = self.initialP3(p)\n",
    "        p2 = self.initialP4(p)\n",
    "        p3 = self.initialP5(p)\n",
    "        mid = torch.cat((p1, p2, p3), 1)\n",
    "        p4 = self.finalP3(mid)\n",
    "        p5 = self.finalP4(mid)\n",
    "        p6 = self.finalP5(mid)\n",
    "        out = torch.cat((p4, p5, p6), 1)\n",
    "        return out\n",
    "\n",
    "# Hiding Network (5 conv layers)\n",
    "class HidingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HidingNetwork, self).__init__()\n",
    "        self.initialH3 = nn.Sequential(\n",
    "            nn.Conv2d(153, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialH4 = nn.Sequential(\n",
    "            nn.Conv2d(153, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialH5 = nn.Sequential(\n",
    "            nn.Conv2d(153, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalH4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH = nn.Sequential(\n",
    "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
    "        \n",
    "    def forward(self, h):\n",
    "        h1 = self.initialH3(h)\n",
    "        h2 = self.initialH4(h)\n",
    "        h3 = self.initialH5(h)\n",
    "        mid = torch.cat((h1, h2, h3), 1)\n",
    "        h4 = self.finalH3(mid)\n",
    "        h5 = self.finalH4(mid)\n",
    "        h6 = self.finalH5(mid)\n",
    "        mid2 = torch.cat((h4, h5, h6), 1)\n",
    "        out = self.finalH(mid2)\n",
    "        out_noise = gaussian(out.data, 0, 0.1)\n",
    "        return out, out_noise\n",
    "\n",
    "# Reveal Network (2 conv layers)\n",
    "class RevealNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RevealNetwork, self).__init__()\n",
    "        self.initialR3 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialR4 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialR5 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalR4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR = nn.Sequential(\n",
    "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, r):\n",
    "        r1 = self.initialR3(r)\n",
    "        r2 = self.initialR4(r)\n",
    "        r3 = self.initialR5(r)\n",
    "        mid = torch.cat((r1, r2, r3), 1)\n",
    "        r4 = self.finalR3(mid)\n",
    "        r5 = self.finalR4(mid)\n",
    "        r6 = self.finalR5(mid)\n",
    "        mid2 = torch.cat((r4, r5, r6), 1)\n",
    "        out = self.finalR(mid2)\n",
    "        return out\n",
    "\n",
    "# Join three networks in one module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.m1 = PrepNetwork()\n",
    "        self.m2 = HidingNetwork()\n",
    "        self.m3 = RevealNetwork()\n",
    "\n",
    "    def forward(self, secret, cover):\n",
    "        x_1 = self.m1(secret)\n",
    "        mid = torch.cat((x_1, cover), 1)\n",
    "        x_2, x_2_noise = self.m2(mid)\n",
    "        x_3 = self.m3(x_2_noise)\n",
    "        return x_2, x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates net object\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create loaders for normalized training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamza_ELF\\anaconda3\\envs\\AI_labs\\lib\\site-packages\\torchvision\\transforms\\transforms.py:279: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "# Creates training set\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(\n",
    "        TRAIN_PATH,\n",
    "        transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean,\n",
    "        std=std)\n",
    "        ])), batch_size=batch_size, num_workers=1, \n",
    "        pin_memory=True, shuffle=True, drop_last=True)\n",
    "\n",
    "# Creates test set\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(\n",
    "        TEST_PATH, \n",
    "        transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean,\n",
    "        std=std)\n",
    "        ])), batch_size=2, num_workers=1, \n",
    "        pin_memory=True, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model and validate it, saving the best model. We use Adam as an optimizer as the paper specifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, beta, learning_rate):\n",
    "    \n",
    "    # Save optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_history = []\n",
    "    # Iterate over batches performing forward and backward passes\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Train mode\n",
    "        net.train()\n",
    "        \n",
    "        train_losses = []\n",
    "        # Train one epoch\n",
    "        for idx, train_batch in enumerate(train_loader):\n",
    "\n",
    "            data, _  = train_batch\n",
    "\n",
    "            # Saves secret images and secret covers\n",
    "            train_covers = data[:len(data)//2]\n",
    "            train_secrets = data[len(data)//2:]\n",
    "            \n",
    "            # Creates variable from secret and cover images\n",
    "            train_secrets = Variable(train_secrets, requires_grad=False)\n",
    "            train_covers = Variable(train_covers, requires_grad=False)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            train_hidden, train_output = net(train_secrets, train_covers)\n",
    "\n",
    "            # Calculate loss and perform backprop\n",
    "            train_loss, train_loss_cover, train_loss_secret = customized_loss(train_output, train_hidden, train_secrets, train_covers, beta)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            # Saves training loss\n",
    "            train_losses.append(train_loss.data)\n",
    "            loss_history.append(train_loss.data)\n",
    "            \n",
    "            # Prints mini-batch losses\n",
    "            print('Training: Batch {0}/{1}. Loss of {2:.4f}, cover loss of {3:.4f}, secret loss of {4:.4f}'.format(idx+1, len(train_loader), train_loss.data, train_loss_cover.data, train_loss_secret.data))\n",
    "    \n",
    "        torch.save(net.state_dict(), MODELS_PATH+'Epoch N{}.pkl'.format(epoch+1))\n",
    "        \n",
    "        mean_train_loss = np.mean(train_losses)\n",
    "    \n",
    "        # Prints epoch average loss\n",
    "        print ('Epoch [{0}/{1}], Average_loss: {2:.4f}'.format(\n",
    "                epoch+1, num_epochs, mean_train_loss))\n",
    "    \n",
    "    return net, mean_train_loss, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamza_ELF\\anaconda3\\envs\\AI_labs\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "<ipython-input-3-723a443b100a>:29: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch 1/250. Loss of 2.5974, cover loss of 0.3314, secret loss of 2.2660\n",
      "Training: Batch 2/250. Loss of 2.8999, cover loss of 1.8116, secret loss of 1.0884\n",
      "Training: Batch 3/250. Loss of 2.4871, cover loss of 1.4669, secret loss of 1.0202\n",
      "Training: Batch 4/250. Loss of 3.4935, cover loss of 1.5476, secret loss of 1.9459\n",
      "Training: Batch 5/250. Loss of 2.5971, cover loss of 1.6447, secret loss of 0.9524\n",
      "Training: Batch 6/250. Loss of 1.2336, cover loss of 0.1927, secret loss of 1.0409\n",
      "Training: Batch 7/250. Loss of 2.1724, cover loss of 0.9694, secret loss of 1.2030\n",
      "Training: Batch 8/250. Loss of 1.1064, cover loss of 0.6157, secret loss of 0.4907\n",
      "Training: Batch 9/250. Loss of 2.4265, cover loss of 1.3744, secret loss of 1.0521\n",
      "Training: Batch 10/250. Loss of 3.6767, cover loss of 1.5092, secret loss of 2.1675\n",
      "Training: Batch 11/250. Loss of 5.0099, cover loss of 2.4308, secret loss of 2.5791\n",
      "Training: Batch 12/250. Loss of 1.5289, cover loss of 0.9231, secret loss of 0.6058\n",
      "Training: Batch 13/250. Loss of 2.2733, cover loss of 0.5555, secret loss of 1.7177\n",
      "Training: Batch 14/250. Loss of 2.1190, cover loss of 1.3656, secret loss of 0.7535\n",
      "Training: Batch 15/250. Loss of 0.9286, cover loss of 0.6983, secret loss of 0.2304\n",
      "Training: Batch 16/250. Loss of 1.8399, cover loss of 1.4184, secret loss of 0.4215\n",
      "Training: Batch 17/250. Loss of 1.8169, cover loss of 0.8147, secret loss of 1.0022\n",
      "Training: Batch 18/250. Loss of 2.3929, cover loss of 1.1565, secret loss of 1.2364\n",
      "Training: Batch 19/250. Loss of 1.2543, cover loss of 0.5215, secret loss of 0.7327\n",
      "Training: Batch 20/250. Loss of 3.1433, cover loss of 1.7541, secret loss of 1.3892\n",
      "Training: Batch 21/250. Loss of 1.2494, cover loss of 0.6700, secret loss of 0.5794\n",
      "Training: Batch 22/250. Loss of 2.3540, cover loss of 0.6770, secret loss of 1.6770\n",
      "Training: Batch 23/250. Loss of 2.6308, cover loss of 1.4250, secret loss of 1.2058\n",
      "Training: Batch 24/250. Loss of 2.2893, cover loss of 0.9771, secret loss of 1.3122\n",
      "Training: Batch 25/250. Loss of 1.7373, cover loss of 0.6306, secret loss of 1.1066\n",
      "Training: Batch 26/250. Loss of 3.8219, cover loss of 0.7880, secret loss of 3.0339\n",
      "Training: Batch 27/250. Loss of 1.9046, cover loss of 0.5697, secret loss of 1.3349\n",
      "Training: Batch 28/250. Loss of 2.3636, cover loss of 0.8206, secret loss of 1.5430\n",
      "Training: Batch 29/250. Loss of 1.8480, cover loss of 0.5013, secret loss of 1.3467\n",
      "Training: Batch 30/250. Loss of 2.4443, cover loss of 1.9715, secret loss of 0.4728\n",
      "Training: Batch 31/250. Loss of 1.8982, cover loss of 0.5043, secret loss of 1.3938\n",
      "Training: Batch 32/250. Loss of 1.9543, cover loss of 0.4698, secret loss of 1.4846\n",
      "Training: Batch 33/250. Loss of 1.7044, cover loss of 0.1734, secret loss of 1.5309\n",
      "Training: Batch 34/250. Loss of 2.1266, cover loss of 0.3546, secret loss of 1.7721\n",
      "Training: Batch 35/250. Loss of 1.5116, cover loss of 0.4647, secret loss of 1.0470\n",
      "Training: Batch 36/250. Loss of 2.2228, cover loss of 0.3697, secret loss of 1.8531\n",
      "Training: Batch 37/250. Loss of 3.0238, cover loss of 1.9963, secret loss of 1.0275\n",
      "Training: Batch 38/250. Loss of 1.7414, cover loss of 0.4104, secret loss of 1.3310\n",
      "Training: Batch 39/250. Loss of 1.6025, cover loss of 0.7681, secret loss of 0.8344\n",
      "Training: Batch 40/250. Loss of 6.3024, cover loss of 3.3306, secret loss of 2.9718\n",
      "Training: Batch 41/250. Loss of 4.0084, cover loss of 0.8635, secret loss of 3.1449\n",
      "Training: Batch 42/250. Loss of 1.7564, cover loss of 0.5022, secret loss of 1.2542\n",
      "Training: Batch 43/250. Loss of 2.4317, cover loss of 0.6139, secret loss of 1.8178\n",
      "Training: Batch 44/250. Loss of 3.0100, cover loss of 1.2903, secret loss of 1.7197\n",
      "Training: Batch 45/250. Loss of 2.7919, cover loss of 0.7940, secret loss of 1.9979\n",
      "Training: Batch 46/250. Loss of 1.7069, cover loss of 0.6796, secret loss of 1.0273\n",
      "Training: Batch 47/250. Loss of 1.7404, cover loss of 0.7680, secret loss of 0.9725\n",
      "Training: Batch 48/250. Loss of 3.6623, cover loss of 1.6402, secret loss of 2.0221\n",
      "Training: Batch 49/250. Loss of 1.6819, cover loss of 0.5367, secret loss of 1.1452\n",
      "Training: Batch 50/250. Loss of 1.7706, cover loss of 0.3818, secret loss of 1.3889\n",
      "Training: Batch 51/250. Loss of 2.4371, cover loss of 0.3543, secret loss of 2.0829\n",
      "Training: Batch 52/250. Loss of 1.4965, cover loss of 0.6487, secret loss of 0.8478\n",
      "Training: Batch 53/250. Loss of 1.9447, cover loss of 1.1601, secret loss of 0.7847\n",
      "Training: Batch 54/250. Loss of 1.8738, cover loss of 0.4041, secret loss of 1.4697\n",
      "Training: Batch 55/250. Loss of 1.9639, cover loss of 0.9278, secret loss of 1.0361\n",
      "Training: Batch 56/250. Loss of 0.9945, cover loss of 0.4156, secret loss of 0.5789\n",
      "Training: Batch 57/250. Loss of 1.7824, cover loss of 0.5516, secret loss of 1.2309\n",
      "Training: Batch 58/250. Loss of 1.6106, cover loss of 1.0195, secret loss of 0.5911\n",
      "Training: Batch 59/250. Loss of 2.8207, cover loss of 0.9719, secret loss of 1.8488\n",
      "Training: Batch 60/250. Loss of 2.6525, cover loss of 0.8193, secret loss of 1.8332\n",
      "Training: Batch 61/250. Loss of 2.2969, cover loss of 0.7275, secret loss of 1.5694\n",
      "Training: Batch 62/250. Loss of 1.1236, cover loss of 0.5365, secret loss of 0.5871\n",
      "Training: Batch 63/250. Loss of 1.8160, cover loss of 0.3565, secret loss of 1.4596\n",
      "Training: Batch 64/250. Loss of 1.6507, cover loss of 0.8173, secret loss of 0.8333\n",
      "Training: Batch 65/250. Loss of 1.7099, cover loss of 0.6688, secret loss of 1.0411\n",
      "Training: Batch 66/250. Loss of 1.4276, cover loss of 0.4292, secret loss of 0.9983\n",
      "Training: Batch 67/250. Loss of 1.2564, cover loss of 0.4448, secret loss of 0.8116\n",
      "Training: Batch 68/250. Loss of 1.3900, cover loss of 0.6846, secret loss of 0.7054\n",
      "Training: Batch 69/250. Loss of 1.3701, cover loss of 0.7288, secret loss of 0.6413\n",
      "Training: Batch 70/250. Loss of 1.3964, cover loss of 0.4056, secret loss of 0.9907\n",
      "Training: Batch 71/250. Loss of 1.4640, cover loss of 0.3095, secret loss of 1.1545\n",
      "Training: Batch 72/250. Loss of 2.0451, cover loss of 0.4806, secret loss of 1.5645\n",
      "Training: Batch 73/250. Loss of 1.8506, cover loss of 0.2362, secret loss of 1.6144\n",
      "Training: Batch 74/250. Loss of 1.5452, cover loss of 0.1316, secret loss of 1.4136\n",
      "Training: Batch 75/250. Loss of 1.8156, cover loss of 0.3462, secret loss of 1.4694\n",
      "Training: Batch 76/250. Loss of 1.5131, cover loss of 0.6574, secret loss of 0.8557\n",
      "Training: Batch 77/250. Loss of 2.5887, cover loss of 0.6769, secret loss of 1.9118\n",
      "Training: Batch 78/250. Loss of 1.5772, cover loss of 0.3415, secret loss of 1.2356\n",
      "Training: Batch 79/250. Loss of 1.4625, cover loss of 0.1523, secret loss of 1.3102\n",
      "Training: Batch 80/250. Loss of 1.0144, cover loss of 0.1407, secret loss of 0.8736\n",
      "Training: Batch 81/250. Loss of 1.1782, cover loss of 0.0935, secret loss of 1.0847\n",
      "Training: Batch 82/250. Loss of 2.3890, cover loss of 0.1851, secret loss of 2.2039\n",
      "Training: Batch 83/250. Loss of 1.4043, cover loss of 0.1083, secret loss of 1.2961\n",
      "Training: Batch 84/250. Loss of 2.2895, cover loss of 0.1002, secret loss of 2.1894\n",
      "Training: Batch 85/250. Loss of 5.7383, cover loss of 0.1413, secret loss of 5.5971\n",
      "Training: Batch 86/250. Loss of 1.1721, cover loss of 0.1163, secret loss of 1.0557\n",
      "Training: Batch 87/250. Loss of 1.1542, cover loss of 0.1260, secret loss of 1.0282\n",
      "Training: Batch 88/250. Loss of 1.1049, cover loss of 0.1055, secret loss of 0.9994\n",
      "Training: Batch 89/250. Loss of 3.8209, cover loss of 0.0921, secret loss of 3.7288\n",
      "Training: Batch 90/250. Loss of 1.0714, cover loss of 0.1350, secret loss of 0.9364\n",
      "Training: Batch 91/250. Loss of 1.2329, cover loss of 0.1065, secret loss of 1.1264\n",
      "Training: Batch 92/250. Loss of 1.1644, cover loss of 0.1962, secret loss of 0.9682\n",
      "Training: Batch 93/250. Loss of 0.7608, cover loss of 0.1809, secret loss of 0.5799\n",
      "Training: Batch 94/250. Loss of 2.7069, cover loss of 0.2655, secret loss of 2.4413\n",
      "Training: Batch 95/250. Loss of 1.0615, cover loss of 0.0926, secret loss of 0.9688\n",
      "Training: Batch 96/250. Loss of 2.6248, cover loss of 0.6922, secret loss of 1.9326\n",
      "Training: Batch 97/250. Loss of 1.7911, cover loss of 0.0632, secret loss of 1.7279\n",
      "Training: Batch 98/250. Loss of 1.1024, cover loss of 0.0703, secret loss of 1.0321\n",
      "Training: Batch 99/250. Loss of 1.0436, cover loss of 0.0850, secret loss of 0.9585\n",
      "Training: Batch 100/250. Loss of 1.6693, cover loss of 0.1393, secret loss of 1.5300\n",
      "Training: Batch 101/250. Loss of 1.4913, cover loss of 0.1921, secret loss of 1.2992\n",
      "Training: Batch 102/250. Loss of 1.3520, cover loss of 0.0466, secret loss of 1.3054\n",
      "Training: Batch 103/250. Loss of 0.9252, cover loss of 0.0531, secret loss of 0.8722\n",
      "Training: Batch 104/250. Loss of 0.7294, cover loss of 0.1200, secret loss of 0.6094\n",
      "Training: Batch 105/250. Loss of 1.5093, cover loss of 0.2585, secret loss of 1.2509\n",
      "Training: Batch 106/250. Loss of 1.2016, cover loss of 0.0940, secret loss of 1.1076\n",
      "Training: Batch 107/250. Loss of 1.1044, cover loss of 0.1294, secret loss of 0.9749\n",
      "Training: Batch 108/250. Loss of 0.6753, cover loss of 0.0896, secret loss of 0.5857\n",
      "Training: Batch 109/250. Loss of 1.1505, cover loss of 0.1222, secret loss of 1.0283\n",
      "Training: Batch 110/250. Loss of 2.0334, cover loss of 0.0994, secret loss of 1.9340\n",
      "Training: Batch 111/250. Loss of 1.0336, cover loss of 0.1567, secret loss of 0.8769\n",
      "Training: Batch 112/250. Loss of 0.7926, cover loss of 0.1086, secret loss of 0.6840\n",
      "Training: Batch 113/250. Loss of 3.1992, cover loss of 0.1414, secret loss of 3.0578\n",
      "Training: Batch 114/250. Loss of 1.3656, cover loss of 0.3004, secret loss of 1.0652\n",
      "Training: Batch 115/250. Loss of 1.0890, cover loss of 0.1146, secret loss of 0.9744\n",
      "Training: Batch 116/250. Loss of 4.7374, cover loss of 0.0969, secret loss of 4.6405\n",
      "Training: Batch 117/250. Loss of 0.7208, cover loss of 0.0637, secret loss of 0.6571\n",
      "Training: Batch 118/250. Loss of 1.2174, cover loss of 0.0593, secret loss of 1.1581\n",
      "Training: Batch 119/250. Loss of 0.9524, cover loss of 0.1080, secret loss of 0.8444\n",
      "Training: Batch 120/250. Loss of 1.5579, cover loss of 0.1012, secret loss of 1.4567\n",
      "Training: Batch 121/250. Loss of 0.8356, cover loss of 0.0866, secret loss of 0.7490\n",
      "Training: Batch 122/250. Loss of 1.0836, cover loss of 0.1332, secret loss of 0.9504\n",
      "Training: Batch 123/250. Loss of 2.5107, cover loss of 0.0598, secret loss of 2.4509\n",
      "Training: Batch 124/250. Loss of 1.7469, cover loss of 0.1670, secret loss of 1.5799\n",
      "Training: Batch 125/250. Loss of 3.6123, cover loss of 0.4259, secret loss of 3.1864\n",
      "Training: Batch 126/250. Loss of 1.6670, cover loss of 0.0532, secret loss of 1.6138\n",
      "Training: Batch 127/250. Loss of 1.7239, cover loss of 0.0768, secret loss of 1.6471\n",
      "Training: Batch 128/250. Loss of 1.1051, cover loss of 0.0554, secret loss of 1.0497\n",
      "Training: Batch 129/250. Loss of 1.3256, cover loss of 0.0440, secret loss of 1.2816\n",
      "Training: Batch 130/250. Loss of 1.4924, cover loss of 0.1147, secret loss of 1.3777\n",
      "Training: Batch 131/250. Loss of 1.0796, cover loss of 0.0366, secret loss of 1.0430\n",
      "Training: Batch 132/250. Loss of 0.7180, cover loss of 0.0824, secret loss of 0.6355\n",
      "Training: Batch 133/250. Loss of 1.7983, cover loss of 0.0822, secret loss of 1.7161\n",
      "Training: Batch 134/250. Loss of 1.2421, cover loss of 0.1560, secret loss of 1.0861\n",
      "Training: Batch 135/250. Loss of 1.7283, cover loss of 0.0361, secret loss of 1.6922\n",
      "Training: Batch 136/250. Loss of 1.2098, cover loss of 0.0752, secret loss of 1.1346\n",
      "Training: Batch 137/250. Loss of 0.7314, cover loss of 0.0472, secret loss of 0.6842\n",
      "Training: Batch 138/250. Loss of 1.0117, cover loss of 0.0331, secret loss of 0.9786\n",
      "Training: Batch 139/250. Loss of 2.1906, cover loss of 0.0768, secret loss of 2.1138\n",
      "Training: Batch 140/250. Loss of 1.6966, cover loss of 0.0767, secret loss of 1.6198\n",
      "Training: Batch 141/250. Loss of 1.4412, cover loss of 0.1113, secret loss of 1.3299\n",
      "Training: Batch 142/250. Loss of 0.9134, cover loss of 0.0402, secret loss of 0.8732\n",
      "Training: Batch 143/250. Loss of 2.6849, cover loss of 0.1724, secret loss of 2.5125\n",
      "Training: Batch 144/250. Loss of 2.1973, cover loss of 0.4917, secret loss of 1.7056\n",
      "Training: Batch 145/250. Loss of 1.8921, cover loss of 0.1532, secret loss of 1.7389\n",
      "Training: Batch 146/250. Loss of 0.5040, cover loss of 0.1146, secret loss of 0.3894\n",
      "Training: Batch 147/250. Loss of 1.0982, cover loss of 0.0622, secret loss of 1.0360\n",
      "Training: Batch 148/250. Loss of 1.4589, cover loss of 0.0596, secret loss of 1.3993\n",
      "Training: Batch 149/250. Loss of 1.0687, cover loss of 0.0585, secret loss of 1.0101\n",
      "Training: Batch 150/250. Loss of 0.6701, cover loss of 0.1570, secret loss of 0.5131\n",
      "Training: Batch 151/250. Loss of 0.6837, cover loss of 0.0921, secret loss of 0.5916\n",
      "Training: Batch 152/250. Loss of 0.7841, cover loss of 0.0362, secret loss of 0.7479\n",
      "Training: Batch 153/250. Loss of 1.2105, cover loss of 0.2210, secret loss of 0.9895\n",
      "Training: Batch 154/250. Loss of 1.3381, cover loss of 0.1392, secret loss of 1.1990\n",
      "Training: Batch 155/250. Loss of 0.5938, cover loss of 0.0874, secret loss of 0.5064\n",
      "Training: Batch 156/250. Loss of 0.6968, cover loss of 0.0402, secret loss of 0.6566\n",
      "Training: Batch 157/250. Loss of 1.4471, cover loss of 0.0642, secret loss of 1.3829\n",
      "Training: Batch 158/250. Loss of 0.8508, cover loss of 0.0649, secret loss of 0.7860\n",
      "Training: Batch 159/250. Loss of 2.0273, cover loss of 0.0344, secret loss of 1.9929\n",
      "Training: Batch 160/250. Loss of 1.2633, cover loss of 0.0258, secret loss of 1.2375\n",
      "Training: Batch 161/250. Loss of 1.2040, cover loss of 0.0784, secret loss of 1.1257\n",
      "Training: Batch 162/250. Loss of 2.0312, cover loss of 0.1747, secret loss of 1.8565\n",
      "Training: Batch 163/250. Loss of 1.1878, cover loss of 0.0677, secret loss of 1.1201\n",
      "Training: Batch 164/250. Loss of 1.4866, cover loss of 0.0632, secret loss of 1.4234\n",
      "Training: Batch 165/250. Loss of 0.7535, cover loss of 0.0596, secret loss of 0.6939\n",
      "Training: Batch 166/250. Loss of 0.4863, cover loss of 0.0687, secret loss of 0.4176\n",
      "Training: Batch 167/250. Loss of 0.7218, cover loss of 0.0792, secret loss of 0.6426\n",
      "Training: Batch 168/250. Loss of 2.4988, cover loss of 1.1757, secret loss of 1.3231\n",
      "Training: Batch 169/250. Loss of 0.9229, cover loss of 0.1068, secret loss of 0.8161\n",
      "Training: Batch 170/250. Loss of 1.1686, cover loss of 0.0510, secret loss of 1.1176\n",
      "Training: Batch 171/250. Loss of 2.1167, cover loss of 0.1244, secret loss of 1.9923\n",
      "Training: Batch 172/250. Loss of 0.8659, cover loss of 0.0419, secret loss of 0.8240\n",
      "Training: Batch 173/250. Loss of 3.0621, cover loss of 0.1567, secret loss of 2.9053\n",
      "Training: Batch 174/250. Loss of 3.7099, cover loss of 0.1270, secret loss of 3.5828\n",
      "Training: Batch 175/250. Loss of 1.0805, cover loss of 0.0733, secret loss of 1.0072\n",
      "Training: Batch 176/250. Loss of 2.2688, cover loss of 0.1266, secret loss of 2.1423\n",
      "Training: Batch 177/250. Loss of 0.7642, cover loss of 0.1426, secret loss of 0.6216\n",
      "Training: Batch 178/250. Loss of 1.8365, cover loss of 0.0759, secret loss of 1.7606\n",
      "Training: Batch 179/250. Loss of 0.9059, cover loss of 0.0361, secret loss of 0.8699\n",
      "Training: Batch 180/250. Loss of 2.4973, cover loss of 0.0762, secret loss of 2.4211\n",
      "Training: Batch 181/250. Loss of 1.8625, cover loss of 0.1174, secret loss of 1.7450\n",
      "Training: Batch 182/250. Loss of 3.7710, cover loss of 0.2730, secret loss of 3.4981\n",
      "Training: Batch 183/250. Loss of 1.4318, cover loss of 0.0562, secret loss of 1.3756\n",
      "Training: Batch 184/250. Loss of 1.4456, cover loss of 0.0688, secret loss of 1.3768\n",
      "Training: Batch 185/250. Loss of 1.6878, cover loss of 0.0862, secret loss of 1.6016\n",
      "Training: Batch 186/250. Loss of 2.2424, cover loss of 0.3124, secret loss of 1.9300\n",
      "Training: Batch 187/250. Loss of 1.4966, cover loss of 0.1568, secret loss of 1.3398\n",
      "Training: Batch 188/250. Loss of 0.7500, cover loss of 0.0671, secret loss of 0.6829\n",
      "Training: Batch 189/250. Loss of 1.1208, cover loss of 0.0363, secret loss of 1.0846\n",
      "Training: Batch 190/250. Loss of 1.2306, cover loss of 0.0779, secret loss of 1.1527\n",
      "Training: Batch 191/250. Loss of 1.5082, cover loss of 0.0629, secret loss of 1.4453\n",
      "Training: Batch 192/250. Loss of 1.7621, cover loss of 0.0577, secret loss of 1.7044\n",
      "Training: Batch 193/250. Loss of 1.0420, cover loss of 0.0618, secret loss of 0.9803\n",
      "Training: Batch 194/250. Loss of 1.1277, cover loss of 0.1560, secret loss of 0.9717\n",
      "Training: Batch 195/250. Loss of 1.7149, cover loss of 0.1355, secret loss of 1.5794\n",
      "Training: Batch 196/250. Loss of 1.3831, cover loss of 0.0623, secret loss of 1.3208\n",
      "Training: Batch 197/250. Loss of 1.0724, cover loss of 0.0234, secret loss of 1.0490\n",
      "Training: Batch 198/250. Loss of 0.3787, cover loss of 0.0590, secret loss of 0.3198\n",
      "Training: Batch 199/250. Loss of 0.9508, cover loss of 0.1234, secret loss of 0.8274\n",
      "Training: Batch 200/250. Loss of 1.5886, cover loss of 0.0512, secret loss of 1.5374\n",
      "Training: Batch 201/250. Loss of 1.3926, cover loss of 0.0711, secret loss of 1.3215\n",
      "Training: Batch 202/250. Loss of 1.6404, cover loss of 0.0541, secret loss of 1.5863\n",
      "Training: Batch 203/250. Loss of 1.9648, cover loss of 0.1021, secret loss of 1.8628\n",
      "Training: Batch 204/250. Loss of 1.7081, cover loss of 0.0259, secret loss of 1.6822\n",
      "Training: Batch 205/250. Loss of 1.3899, cover loss of 0.0494, secret loss of 1.3405\n",
      "Training: Batch 206/250. Loss of 1.0407, cover loss of 0.1188, secret loss of 0.9218\n",
      "Training: Batch 207/250. Loss of 1.0167, cover loss of 0.0316, secret loss of 0.9851\n",
      "Training: Batch 208/250. Loss of 1.1824, cover loss of 0.1303, secret loss of 1.0521\n",
      "Training: Batch 209/250. Loss of 2.2890, cover loss of 0.0885, secret loss of 2.2005\n",
      "Training: Batch 210/250. Loss of 1.5358, cover loss of 0.2685, secret loss of 1.2673\n",
      "Training: Batch 211/250. Loss of 1.5426, cover loss of 0.0405, secret loss of 1.5021\n",
      "Training: Batch 212/250. Loss of 1.3122, cover loss of 0.1314, secret loss of 1.1808\n",
      "Training: Batch 213/250. Loss of 1.3728, cover loss of 0.0393, secret loss of 1.3334\n",
      "Training: Batch 214/250. Loss of 0.6351, cover loss of 0.1227, secret loss of 0.5125\n",
      "Training: Batch 215/250. Loss of 1.1634, cover loss of 0.0457, secret loss of 1.1178\n",
      "Training: Batch 216/250. Loss of 0.7630, cover loss of 0.0991, secret loss of 0.6639\n",
      "Training: Batch 217/250. Loss of 0.9042, cover loss of 0.0669, secret loss of 0.8372\n",
      "Training: Batch 218/250. Loss of 0.7269, cover loss of 0.0320, secret loss of 0.6949\n",
      "Training: Batch 219/250. Loss of 1.2003, cover loss of 0.0317, secret loss of 1.1686\n",
      "Training: Batch 220/250. Loss of 2.0017, cover loss of 0.3133, secret loss of 1.6885\n",
      "Training: Batch 221/250. Loss of 1.0616, cover loss of 0.0306, secret loss of 1.0310\n",
      "Training: Batch 222/250. Loss of 1.2885, cover loss of 0.0441, secret loss of 1.2444\n",
      "Training: Batch 223/250. Loss of 0.8966, cover loss of 0.0714, secret loss of 0.8251\n",
      "Training: Batch 224/250. Loss of 5.2679, cover loss of 0.0609, secret loss of 5.2070\n",
      "Training: Batch 225/250. Loss of 1.8063, cover loss of 0.0669, secret loss of 1.7394\n",
      "Training: Batch 226/250. Loss of 1.1458, cover loss of 0.0407, secret loss of 1.1051\n",
      "Training: Batch 227/250. Loss of 1.5681, cover loss of 0.0887, secret loss of 1.4794\n",
      "Training: Batch 228/250. Loss of 0.6389, cover loss of 0.0523, secret loss of 0.5866\n",
      "Training: Batch 229/250. Loss of 1.9852, cover loss of 0.0344, secret loss of 1.9507\n",
      "Training: Batch 230/250. Loss of 1.0780, cover loss of 0.3627, secret loss of 0.7153\n",
      "Training: Batch 231/250. Loss of 1.0163, cover loss of 0.1944, secret loss of 0.8219\n",
      "Training: Batch 232/250. Loss of 1.1863, cover loss of 0.0582, secret loss of 1.1281\n",
      "Training: Batch 233/250. Loss of 1.0739, cover loss of 0.0439, secret loss of 1.0301\n",
      "Training: Batch 234/250. Loss of 1.2258, cover loss of 0.0191, secret loss of 1.2066\n",
      "Training: Batch 235/250. Loss of 1.5480, cover loss of 0.0349, secret loss of 1.5131\n",
      "Training: Batch 236/250. Loss of 1.2113, cover loss of 0.0602, secret loss of 1.1511\n",
      "Training: Batch 237/250. Loss of 0.9071, cover loss of 0.1308, secret loss of 0.7763\n",
      "Training: Batch 238/250. Loss of 1.0193, cover loss of 0.2042, secret loss of 0.8151\n",
      "Training: Batch 239/250. Loss of 1.0022, cover loss of 0.1369, secret loss of 0.8653\n",
      "Training: Batch 240/250. Loss of 2.0119, cover loss of 0.0205, secret loss of 1.9914\n",
      "Training: Batch 241/250. Loss of 0.8252, cover loss of 0.0140, secret loss of 0.8112\n",
      "Training: Batch 242/250. Loss of 5.3650, cover loss of 0.0806, secret loss of 5.2844\n",
      "Training: Batch 243/250. Loss of 1.2232, cover loss of 0.0690, secret loss of 1.1542\n",
      "Training: Batch 244/250. Loss of 0.7069, cover loss of 0.0901, secret loss of 0.6168\n",
      "Training: Batch 245/250. Loss of 1.1707, cover loss of 0.0185, secret loss of 1.1522\n",
      "Training: Batch 246/250. Loss of 0.9911, cover loss of 0.0878, secret loss of 0.9033\n",
      "Training: Batch 247/250. Loss of 0.5913, cover loss of 0.0513, secret loss of 0.5400\n",
      "Training: Batch 248/250. Loss of 1.0762, cover loss of 0.0566, secret loss of 1.0196\n",
      "Training: Batch 249/250. Loss of 2.2056, cover loss of 0.2287, secret loss of 1.9769\n",
      "Training: Batch 250/250. Loss of 2.7199, cover loss of 0.0308, secret loss of 2.6891\n",
      "Epoch [1/3], Average_loss: 1.6983\n",
      "Training: Batch 1/250. Loss of 1.0569, cover loss of 0.0412, secret loss of 1.0157\n",
      "Training: Batch 2/250. Loss of 0.8617, cover loss of 0.0168, secret loss of 0.8450\n",
      "Training: Batch 3/250. Loss of 1.3794, cover loss of 0.1629, secret loss of 1.2165\n",
      "Training: Batch 4/250. Loss of 0.9427, cover loss of 0.1531, secret loss of 0.7896\n",
      "Training: Batch 5/250. Loss of 0.5339, cover loss of 0.0922, secret loss of 0.4417\n",
      "Training: Batch 6/250. Loss of 0.8958, cover loss of 0.0532, secret loss of 0.8426\n",
      "Training: Batch 7/250. Loss of 0.6048, cover loss of 0.0304, secret loss of 0.5744\n",
      "Training: Batch 8/250. Loss of 0.9583, cover loss of 0.0879, secret loss of 0.8704\n",
      "Training: Batch 9/250. Loss of 1.4966, cover loss of 0.0467, secret loss of 1.4498\n",
      "Training: Batch 10/250. Loss of 1.5065, cover loss of 0.1052, secret loss of 1.4013\n",
      "Training: Batch 11/250. Loss of 0.9549, cover loss of 0.0368, secret loss of 0.9181\n",
      "Training: Batch 12/250. Loss of 0.6848, cover loss of 0.0535, secret loss of 0.6313\n",
      "Training: Batch 13/250. Loss of 0.8338, cover loss of 0.1439, secret loss of 0.6899\n",
      "Training: Batch 14/250. Loss of 0.7182, cover loss of 0.0379, secret loss of 0.6804\n",
      "Training: Batch 15/250. Loss of 1.5615, cover loss of 0.1165, secret loss of 1.4451\n",
      "Training: Batch 16/250. Loss of 1.8847, cover loss of 0.0496, secret loss of 1.8351\n",
      "Training: Batch 17/250. Loss of 1.2239, cover loss of 0.0805, secret loss of 1.1434\n",
      "Training: Batch 18/250. Loss of 0.8656, cover loss of 0.0319, secret loss of 0.8337\n",
      "Training: Batch 19/250. Loss of 0.8506, cover loss of 0.0416, secret loss of 0.8090\n",
      "Training: Batch 20/250. Loss of 1.5520, cover loss of 0.0291, secret loss of 1.5229\n",
      "Training: Batch 21/250. Loss of 1.2218, cover loss of 0.0844, secret loss of 1.1374\n",
      "Training: Batch 22/250. Loss of 1.7149, cover loss of 0.1239, secret loss of 1.5910\n",
      "Training: Batch 23/250. Loss of 1.5576, cover loss of 0.0984, secret loss of 1.4592\n",
      "Training: Batch 24/250. Loss of 1.2089, cover loss of 0.0968, secret loss of 1.1121\n",
      "Training: Batch 25/250. Loss of 1.6412, cover loss of 0.0196, secret loss of 1.6216\n",
      "Training: Batch 26/250. Loss of 1.4833, cover loss of 0.2725, secret loss of 1.2108\n",
      "Training: Batch 27/250. Loss of 0.9468, cover loss of 0.0567, secret loss of 0.8901\n",
      "Training: Batch 28/250. Loss of 1.7118, cover loss of 0.0291, secret loss of 1.6827\n",
      "Training: Batch 29/250. Loss of 1.3105, cover loss of 0.0156, secret loss of 1.2949\n",
      "Training: Batch 30/250. Loss of 0.8648, cover loss of 0.0610, secret loss of 0.8038\n",
      "Training: Batch 31/250. Loss of 1.5586, cover loss of 0.0234, secret loss of 1.5352\n",
      "Training: Batch 32/250. Loss of 0.9263, cover loss of 0.0826, secret loss of 0.8437\n",
      "Training: Batch 33/250. Loss of 2.0580, cover loss of 0.0350, secret loss of 2.0229\n",
      "Training: Batch 34/250. Loss of 1.0749, cover loss of 0.0283, secret loss of 1.0466\n",
      "Training: Batch 35/250. Loss of 1.1799, cover loss of 0.0875, secret loss of 1.0924\n",
      "Training: Batch 36/250. Loss of 0.8823, cover loss of 0.0399, secret loss of 0.8424\n",
      "Training: Batch 37/250. Loss of 0.7364, cover loss of 0.1251, secret loss of 0.6113\n",
      "Training: Batch 38/250. Loss of 1.0226, cover loss of 0.0512, secret loss of 0.9714\n",
      "Training: Batch 39/250. Loss of 1.2550, cover loss of 0.0255, secret loss of 1.2295\n",
      "Training: Batch 40/250. Loss of 1.4337, cover loss of 0.0236, secret loss of 1.4100\n",
      "Training: Batch 41/250. Loss of 0.9722, cover loss of 0.0631, secret loss of 0.9091\n",
      "Training: Batch 42/250. Loss of 2.0842, cover loss of 0.0735, secret loss of 2.0107\n",
      "Training: Batch 43/250. Loss of 1.1740, cover loss of 0.0524, secret loss of 1.1216\n",
      "Training: Batch 44/250. Loss of 2.0222, cover loss of 0.0280, secret loss of 1.9942\n",
      "Training: Batch 45/250. Loss of 3.8689, cover loss of 0.0312, secret loss of 3.8377\n",
      "Training: Batch 46/250. Loss of 0.9771, cover loss of 0.0557, secret loss of 0.9214\n",
      "Training: Batch 47/250. Loss of 1.7522, cover loss of 0.0293, secret loss of 1.7229\n",
      "Training: Batch 48/250. Loss of 0.7352, cover loss of 0.0179, secret loss of 0.7173\n",
      "Training: Batch 49/250. Loss of 0.9870, cover loss of 0.0874, secret loss of 0.8996\n",
      "Training: Batch 50/250. Loss of 1.5285, cover loss of 0.0767, secret loss of 1.4517\n",
      "Training: Batch 51/250. Loss of 0.7058, cover loss of 0.1355, secret loss of 0.5703\n",
      "Training: Batch 52/250. Loss of 1.0741, cover loss of 0.0774, secret loss of 0.9967\n",
      "Training: Batch 53/250. Loss of 0.7495, cover loss of 0.0210, secret loss of 0.7285\n",
      "Training: Batch 54/250. Loss of 1.6905, cover loss of 0.0635, secret loss of 1.6270\n",
      "Training: Batch 55/250. Loss of 0.3527, cover loss of 0.0827, secret loss of 0.2701\n",
      "Training: Batch 56/250. Loss of 1.0103, cover loss of 0.0426, secret loss of 0.9677\n",
      "Training: Batch 57/250. Loss of 0.4643, cover loss of 0.2161, secret loss of 0.2482\n",
      "Training: Batch 58/250. Loss of 1.0673, cover loss of 0.0842, secret loss of 0.9832\n",
      "Training: Batch 59/250. Loss of 1.0571, cover loss of 0.0196, secret loss of 1.0375\n",
      "Training: Batch 60/250. Loss of 1.8762, cover loss of 0.0220, secret loss of 1.8542\n",
      "Training: Batch 61/250. Loss of 0.7896, cover loss of 0.0355, secret loss of 0.7541\n",
      "Training: Batch 62/250. Loss of 1.2379, cover loss of 0.0688, secret loss of 1.1691\n",
      "Training: Batch 63/250. Loss of 0.5716, cover loss of 0.0625, secret loss of 0.5092\n",
      "Training: Batch 64/250. Loss of 0.7124, cover loss of 0.0556, secret loss of 0.6568\n",
      "Training: Batch 65/250. Loss of 0.8334, cover loss of 0.0637, secret loss of 0.7698\n",
      "Training: Batch 66/250. Loss of 0.5985, cover loss of 0.0856, secret loss of 0.5129\n",
      "Training: Batch 67/250. Loss of 1.1829, cover loss of 0.0327, secret loss of 1.1502\n",
      "Training: Batch 68/250. Loss of 0.9651, cover loss of 0.0379, secret loss of 0.9272\n",
      "Training: Batch 69/250. Loss of 1.5281, cover loss of 0.0526, secret loss of 1.4755\n",
      "Training: Batch 70/250. Loss of 1.1867, cover loss of 0.1324, secret loss of 1.0543\n",
      "Training: Batch 71/250. Loss of 1.0595, cover loss of 0.0523, secret loss of 1.0072\n",
      "Training: Batch 72/250. Loss of 0.7095, cover loss of 0.0536, secret loss of 0.6559\n",
      "Training: Batch 73/250. Loss of 1.1596, cover loss of 0.0758, secret loss of 1.0838\n",
      "Training: Batch 74/250. Loss of 0.5269, cover loss of 0.0208, secret loss of 0.5061\n",
      "Training: Batch 75/250. Loss of 1.3743, cover loss of 0.1214, secret loss of 1.2529\n",
      "Training: Batch 76/250. Loss of 1.2287, cover loss of 0.0170, secret loss of 1.2117\n",
      "Training: Batch 77/250. Loss of 0.8135, cover loss of 0.0299, secret loss of 0.7837\n",
      "Training: Batch 78/250. Loss of 1.7718, cover loss of 0.0320, secret loss of 1.7399\n",
      "Training: Batch 79/250. Loss of 1.1096, cover loss of 0.0222, secret loss of 1.0874\n",
      "Training: Batch 80/250. Loss of 1.5970, cover loss of 0.0506, secret loss of 1.5464\n",
      "Training: Batch 81/250. Loss of 1.8198, cover loss of 0.0379, secret loss of 1.7819\n",
      "Training: Batch 82/250. Loss of 1.2746, cover loss of 0.0558, secret loss of 1.2189\n",
      "Training: Batch 83/250. Loss of 1.3490, cover loss of 0.1051, secret loss of 1.2439\n",
      "Training: Batch 84/250. Loss of 0.7178, cover loss of 0.0640, secret loss of 0.6538\n",
      "Training: Batch 85/250. Loss of 0.9553, cover loss of 0.0265, secret loss of 0.9289\n",
      "Training: Batch 86/250. Loss of 0.6206, cover loss of 0.0390, secret loss of 0.5816\n",
      "Training: Batch 87/250. Loss of 1.5225, cover loss of 0.0354, secret loss of 1.4870\n",
      "Training: Batch 88/250. Loss of 0.4618, cover loss of 0.0177, secret loss of 0.4441\n",
      "Training: Batch 89/250. Loss of 0.8078, cover loss of 0.0936, secret loss of 0.7143\n",
      "Training: Batch 90/250. Loss of 1.0458, cover loss of 0.0718, secret loss of 0.9740\n",
      "Training: Batch 91/250. Loss of 1.3012, cover loss of 0.0651, secret loss of 1.2361\n",
      "Training: Batch 92/250. Loss of 3.5222, cover loss of 0.0409, secret loss of 3.4813\n",
      "Training: Batch 93/250. Loss of 0.6090, cover loss of 0.0217, secret loss of 0.5873\n",
      "Training: Batch 94/250. Loss of 0.7886, cover loss of 0.0329, secret loss of 0.7556\n",
      "Training: Batch 95/250. Loss of 1.0264, cover loss of 0.0775, secret loss of 0.9489\n",
      "Training: Batch 96/250. Loss of 1.2257, cover loss of 0.0446, secret loss of 1.1810\n",
      "Training: Batch 97/250. Loss of 0.7890, cover loss of 0.0406, secret loss of 0.7483\n",
      "Training: Batch 98/250. Loss of 0.4875, cover loss of 0.1850, secret loss of 0.3025\n",
      "Training: Batch 99/250. Loss of 1.0199, cover loss of 0.0269, secret loss of 0.9930\n",
      "Training: Batch 100/250. Loss of 0.6071, cover loss of 0.0268, secret loss of 0.5803\n",
      "Training: Batch 101/250. Loss of 1.7116, cover loss of 0.0605, secret loss of 1.6511\n",
      "Training: Batch 102/250. Loss of 1.3594, cover loss of 0.0308, secret loss of 1.3286\n",
      "Training: Batch 103/250. Loss of 1.2327, cover loss of 0.0386, secret loss of 1.1941\n",
      "Training: Batch 104/250. Loss of 0.8817, cover loss of 0.0184, secret loss of 0.8633\n",
      "Training: Batch 105/250. Loss of 2.1190, cover loss of 0.0496, secret loss of 2.0694\n",
      "Training: Batch 106/250. Loss of 0.4471, cover loss of 0.0343, secret loss of 0.4128\n",
      "Training: Batch 107/250. Loss of 1.5273, cover loss of 0.0096, secret loss of 1.5177\n",
      "Training: Batch 108/250. Loss of 1.2499, cover loss of 0.0331, secret loss of 1.2168\n",
      "Training: Batch 109/250. Loss of 1.4254, cover loss of 0.0093, secret loss of 1.4161\n",
      "Training: Batch 110/250. Loss of 2.3405, cover loss of 0.1878, secret loss of 2.1527\n",
      "Training: Batch 111/250. Loss of 1.7987, cover loss of 0.0573, secret loss of 1.7415\n",
      "Training: Batch 112/250. Loss of 1.5053, cover loss of 0.0168, secret loss of 1.4886\n",
      "Training: Batch 113/250. Loss of 2.6648, cover loss of 0.0297, secret loss of 2.6351\n",
      "Training: Batch 114/250. Loss of 0.6385, cover loss of 0.0191, secret loss of 0.6195\n",
      "Training: Batch 115/250. Loss of 1.1272, cover loss of 0.0261, secret loss of 1.1011\n",
      "Training: Batch 116/250. Loss of 1.4004, cover loss of 0.0233, secret loss of 1.3771\n",
      "Training: Batch 117/250. Loss of 1.1366, cover loss of 0.0268, secret loss of 1.1098\n",
      "Training: Batch 118/250. Loss of 0.5969, cover loss of 0.0300, secret loss of 0.5669\n",
      "Training: Batch 119/250. Loss of 0.6725, cover loss of 0.0422, secret loss of 0.6303\n",
      "Training: Batch 120/250. Loss of 0.4743, cover loss of 0.0355, secret loss of 0.4388\n",
      "Training: Batch 121/250. Loss of 1.5659, cover loss of 0.0557, secret loss of 1.5103\n",
      "Training: Batch 122/250. Loss of 0.8794, cover loss of 0.0152, secret loss of 0.8642\n",
      "Training: Batch 123/250. Loss of 1.0582, cover loss of 0.0177, secret loss of 1.0405\n",
      "Training: Batch 124/250. Loss of 0.7115, cover loss of 0.0993, secret loss of 0.6122\n",
      "Training: Batch 125/250. Loss of 0.4061, cover loss of 0.0398, secret loss of 0.3663\n",
      "Training: Batch 126/250. Loss of 0.7541, cover loss of 0.0490, secret loss of 0.7051\n",
      "Training: Batch 127/250. Loss of 1.2191, cover loss of 0.0288, secret loss of 1.1903\n",
      "Training: Batch 128/250. Loss of 0.4260, cover loss of 0.0154, secret loss of 0.4106\n",
      "Training: Batch 129/250. Loss of 0.9625, cover loss of 0.0417, secret loss of 0.9208\n",
      "Training: Batch 130/250. Loss of 2.2585, cover loss of 0.0398, secret loss of 2.2187\n",
      "Training: Batch 131/250. Loss of 0.9148, cover loss of 0.1259, secret loss of 0.7889\n",
      "Training: Batch 132/250. Loss of 0.8780, cover loss of 0.0421, secret loss of 0.8359\n",
      "Training: Batch 133/250. Loss of 0.6868, cover loss of 0.0334, secret loss of 0.6534\n",
      "Training: Batch 134/250. Loss of 0.9516, cover loss of 0.0436, secret loss of 0.9080\n",
      "Training: Batch 135/250. Loss of 1.4517, cover loss of 0.0557, secret loss of 1.3960\n",
      "Training: Batch 136/250. Loss of 3.6961, cover loss of 0.0463, secret loss of 3.6498\n",
      "Training: Batch 137/250. Loss of 0.7645, cover loss of 0.0335, secret loss of 0.7310\n",
      "Training: Batch 138/250. Loss of 0.8308, cover loss of 0.0251, secret loss of 0.8057\n",
      "Training: Batch 139/250. Loss of 1.5159, cover loss of 0.0234, secret loss of 1.4924\n",
      "Training: Batch 140/250. Loss of 1.0577, cover loss of 0.0135, secret loss of 1.0442\n",
      "Training: Batch 141/250. Loss of 1.0065, cover loss of 0.0821, secret loss of 0.9243\n",
      "Training: Batch 142/250. Loss of 0.8084, cover loss of 0.0156, secret loss of 0.7928\n",
      "Training: Batch 143/250. Loss of 0.7876, cover loss of 0.0225, secret loss of 0.7651\n",
      "Training: Batch 144/250. Loss of 1.3341, cover loss of 0.0217, secret loss of 1.3123\n",
      "Training: Batch 145/250. Loss of 5.3042, cover loss of 0.0207, secret loss of 5.2835\n",
      "Training: Batch 146/250. Loss of 1.7321, cover loss of 0.0257, secret loss of 1.7064\n",
      "Training: Batch 147/250. Loss of 0.7365, cover loss of 0.0383, secret loss of 0.6982\n",
      "Training: Batch 148/250. Loss of 2.7789, cover loss of 0.0259, secret loss of 2.7530\n",
      "Training: Batch 149/250. Loss of 0.9631, cover loss of 0.0239, secret loss of 0.9392\n",
      "Training: Batch 150/250. Loss of 1.3153, cover loss of 0.0213, secret loss of 1.2940\n",
      "Training: Batch 151/250. Loss of 3.7749, cover loss of 0.0647, secret loss of 3.7102\n",
      "Training: Batch 152/250. Loss of 3.1016, cover loss of 0.1777, secret loss of 2.9239\n",
      "Training: Batch 153/250. Loss of 1.1352, cover loss of 0.0372, secret loss of 1.0980\n",
      "Training: Batch 154/250. Loss of 1.6730, cover loss of 0.0147, secret loss of 1.6583\n",
      "Training: Batch 155/250. Loss of 1.7189, cover loss of 0.0695, secret loss of 1.6494\n",
      "Training: Batch 156/250. Loss of 2.2000, cover loss of 0.0301, secret loss of 2.1699\n",
      "Training: Batch 157/250. Loss of 1.7948, cover loss of 0.0260, secret loss of 1.7688\n",
      "Training: Batch 158/250. Loss of 0.5858, cover loss of 0.0333, secret loss of 0.5525\n",
      "Training: Batch 159/250. Loss of 1.2317, cover loss of 0.0729, secret loss of 1.1588\n",
      "Training: Batch 160/250. Loss of 0.9127, cover loss of 0.0170, secret loss of 0.8957\n",
      "Training: Batch 161/250. Loss of 1.5659, cover loss of 0.0272, secret loss of 1.5387\n",
      "Training: Batch 162/250. Loss of 1.1915, cover loss of 0.0971, secret loss of 1.0944\n",
      "Training: Batch 163/250. Loss of 1.2667, cover loss of 0.0383, secret loss of 1.2284\n",
      "Training: Batch 164/250. Loss of 1.0155, cover loss of 0.0183, secret loss of 0.9972\n",
      "Training: Batch 165/250. Loss of 1.8258, cover loss of 0.0339, secret loss of 1.7919\n",
      "Training: Batch 166/250. Loss of 1.2296, cover loss of 0.0487, secret loss of 1.1809\n",
      "Training: Batch 167/250. Loss of 1.0215, cover loss of 0.0177, secret loss of 1.0038\n",
      "Training: Batch 168/250. Loss of 1.1633, cover loss of 0.0468, secret loss of 1.1165\n",
      "Training: Batch 169/250. Loss of 1.1917, cover loss of 0.0175, secret loss of 1.1742\n",
      "Training: Batch 170/250. Loss of 0.8785, cover loss of 0.0943, secret loss of 0.7843\n",
      "Training: Batch 171/250. Loss of 1.5424, cover loss of 0.0342, secret loss of 1.5083\n",
      "Training: Batch 172/250. Loss of 1.3458, cover loss of 0.0071, secret loss of 1.3387\n",
      "Training: Batch 173/250. Loss of 0.6081, cover loss of 0.0401, secret loss of 0.5680\n",
      "Training: Batch 174/250. Loss of 1.6659, cover loss of 0.0330, secret loss of 1.6329\n",
      "Training: Batch 175/250. Loss of 0.7068, cover loss of 0.0346, secret loss of 0.6723\n",
      "Training: Batch 176/250. Loss of 0.9700, cover loss of 0.0611, secret loss of 0.9088\n",
      "Training: Batch 177/250. Loss of 0.9906, cover loss of 0.0350, secret loss of 0.9556\n",
      "Training: Batch 178/250. Loss of 0.8735, cover loss of 0.0562, secret loss of 0.8173\n",
      "Training: Batch 179/250. Loss of 0.4082, cover loss of 0.0260, secret loss of 0.3822\n",
      "Training: Batch 180/250. Loss of 1.5640, cover loss of 0.0381, secret loss of 1.5259\n",
      "Training: Batch 181/250. Loss of 1.5160, cover loss of 0.0665, secret loss of 1.4495\n",
      "Training: Batch 182/250. Loss of 0.7498, cover loss of 0.0336, secret loss of 0.7163\n",
      "Training: Batch 183/250. Loss of 1.2150, cover loss of 0.0336, secret loss of 1.1814\n",
      "Training: Batch 184/250. Loss of 1.9822, cover loss of 0.0755, secret loss of 1.9067\n",
      "Training: Batch 185/250. Loss of 1.0989, cover loss of 0.0307, secret loss of 1.0682\n",
      "Training: Batch 186/250. Loss of 1.1389, cover loss of 0.0303, secret loss of 1.1086\n",
      "Training: Batch 187/250. Loss of 0.7589, cover loss of 0.0388, secret loss of 0.7200\n",
      "Training: Batch 188/250. Loss of 1.0301, cover loss of 0.0196, secret loss of 1.0104\n",
      "Training: Batch 189/250. Loss of 1.1505, cover loss of 0.0485, secret loss of 1.1020\n",
      "Training: Batch 190/250. Loss of 1.1490, cover loss of 0.0297, secret loss of 1.1193\n",
      "Training: Batch 191/250. Loss of 3.7419, cover loss of 0.0513, secret loss of 3.6905\n",
      "Training: Batch 192/250. Loss of 1.2868, cover loss of 0.0413, secret loss of 1.2455\n",
      "Training: Batch 193/250. Loss of 0.9803, cover loss of 0.0201, secret loss of 0.9603\n",
      "Training: Batch 194/250. Loss of 1.0891, cover loss of 0.0209, secret loss of 1.0681\n",
      "Training: Batch 195/250. Loss of 1.0028, cover loss of 0.0245, secret loss of 0.9782\n",
      "Training: Batch 196/250. Loss of 1.4259, cover loss of 0.0175, secret loss of 1.4083\n",
      "Training: Batch 197/250. Loss of 0.8277, cover loss of 0.0648, secret loss of 0.7629\n",
      "Training: Batch 198/250. Loss of 1.7773, cover loss of 0.0244, secret loss of 1.7530\n",
      "Training: Batch 199/250. Loss of 0.4503, cover loss of 0.0235, secret loss of 0.4267\n",
      "Training: Batch 200/250. Loss of 1.9051, cover loss of 0.0322, secret loss of 1.8729\n",
      "Training: Batch 201/250. Loss of 0.7566, cover loss of 0.0260, secret loss of 0.7306\n",
      "Training: Batch 202/250. Loss of 1.1398, cover loss of 0.0176, secret loss of 1.1222\n",
      "Training: Batch 203/250. Loss of 1.2489, cover loss of 0.0425, secret loss of 1.2064\n",
      "Training: Batch 204/250. Loss of 1.1108, cover loss of 0.0240, secret loss of 1.0868\n",
      "Training: Batch 205/250. Loss of 2.0207, cover loss of 0.0240, secret loss of 1.9967\n",
      "Training: Batch 206/250. Loss of 1.7740, cover loss of 0.0198, secret loss of 1.7542\n",
      "Training: Batch 207/250. Loss of 2.2173, cover loss of 0.0270, secret loss of 2.1903\n",
      "Training: Batch 208/250. Loss of 1.1230, cover loss of 0.0599, secret loss of 1.0631\n",
      "Training: Batch 209/250. Loss of 0.6371, cover loss of 0.0307, secret loss of 0.6064\n",
      "Training: Batch 210/250. Loss of 5.6787, cover loss of 0.0293, secret loss of 5.6493\n",
      "Training: Batch 211/250. Loss of 0.5352, cover loss of 0.0268, secret loss of 0.5084\n",
      "Training: Batch 212/250. Loss of 2.1557, cover loss of 0.0298, secret loss of 2.1259\n",
      "Training: Batch 213/250. Loss of 1.1762, cover loss of 0.0212, secret loss of 1.1550\n",
      "Training: Batch 214/250. Loss of 1.1912, cover loss of 0.0319, secret loss of 1.1593\n",
      "Training: Batch 215/250. Loss of 0.7990, cover loss of 0.0240, secret loss of 0.7750\n",
      "Training: Batch 216/250. Loss of 1.5327, cover loss of 0.0170, secret loss of 1.5156\n",
      "Training: Batch 217/250. Loss of 0.6523, cover loss of 0.0156, secret loss of 0.6367\n",
      "Training: Batch 218/250. Loss of 1.8114, cover loss of 0.0086, secret loss of 1.8028\n",
      "Training: Batch 219/250. Loss of 1.2490, cover loss of 0.0173, secret loss of 1.2316\n",
      "Training: Batch 220/250. Loss of 1.6917, cover loss of 0.0272, secret loss of 1.6645\n",
      "Training: Batch 221/250. Loss of 1.6077, cover loss of 0.0178, secret loss of 1.5899\n",
      "Training: Batch 222/250. Loss of 1.4459, cover loss of 0.0124, secret loss of 1.4335\n",
      "Training: Batch 223/250. Loss of 0.5984, cover loss of 0.0158, secret loss of 0.5826\n",
      "Training: Batch 224/250. Loss of 0.7183, cover loss of 0.0117, secret loss of 0.7066\n",
      "Training: Batch 225/250. Loss of 1.1052, cover loss of 0.0468, secret loss of 1.0584\n",
      "Training: Batch 226/250. Loss of 3.2805, cover loss of 0.0174, secret loss of 3.2631\n",
      "Training: Batch 227/250. Loss of 1.4480, cover loss of 0.0333, secret loss of 1.4147\n",
      "Training: Batch 228/250. Loss of 1.3587, cover loss of 0.0112, secret loss of 1.3475\n",
      "Training: Batch 229/250. Loss of 0.6110, cover loss of 0.0478, secret loss of 0.5632\n",
      "Training: Batch 230/250. Loss of 1.9268, cover loss of 0.0200, secret loss of 1.9068\n",
      "Training: Batch 231/250. Loss of 0.9322, cover loss of 0.0290, secret loss of 0.9032\n",
      "Training: Batch 232/250. Loss of 0.8980, cover loss of 0.0211, secret loss of 0.8769\n",
      "Training: Batch 233/250. Loss of 0.5005, cover loss of 0.0221, secret loss of 0.4784\n",
      "Training: Batch 234/250. Loss of 0.3773, cover loss of 0.0246, secret loss of 0.3527\n",
      "Training: Batch 235/250. Loss of 1.4330, cover loss of 0.0180, secret loss of 1.4150\n",
      "Training: Batch 236/250. Loss of 2.9018, cover loss of 0.0226, secret loss of 2.8792\n",
      "Training: Batch 237/250. Loss of 0.9336, cover loss of 0.0181, secret loss of 0.9156\n",
      "Training: Batch 238/250. Loss of 0.6326, cover loss of 0.0200, secret loss of 0.6126\n",
      "Training: Batch 239/250. Loss of 0.1360, cover loss of 0.0102, secret loss of 0.1257\n",
      "Training: Batch 240/250. Loss of 3.1855, cover loss of 0.0160, secret loss of 3.1695\n",
      "Training: Batch 241/250. Loss of 1.4830, cover loss of 0.0074, secret loss of 1.4756\n",
      "Training: Batch 242/250. Loss of 2.3337, cover loss of 0.0159, secret loss of 2.3178\n",
      "Training: Batch 243/250. Loss of 1.1691, cover loss of 0.0388, secret loss of 1.1303\n",
      "Training: Batch 244/250. Loss of 0.9790, cover loss of 0.0124, secret loss of 0.9665\n",
      "Training: Batch 245/250. Loss of 0.6308, cover loss of 0.0131, secret loss of 0.6177\n",
      "Training: Batch 246/250. Loss of 1.9916, cover loss of 0.0084, secret loss of 1.9832\n",
      "Training: Batch 247/250. Loss of 1.3370, cover loss of 0.0241, secret loss of 1.3129\n",
      "Training: Batch 248/250. Loss of 0.5638, cover loss of 0.0213, secret loss of 0.5425\n",
      "Training: Batch 249/250. Loss of 0.4775, cover loss of 0.0175, secret loss of 0.4599\n",
      "Training: Batch 250/250. Loss of 1.3068, cover loss of 0.0146, secret loss of 1.2922\n",
      "Epoch [2/3], Average_loss: 1.2741\n",
      "Training: Batch 1/250. Loss of 3.3969, cover loss of 0.0171, secret loss of 3.3798\n",
      "Training: Batch 2/250. Loss of 2.0910, cover loss of 0.0331, secret loss of 2.0579\n",
      "Training: Batch 3/250. Loss of 1.0429, cover loss of 0.0151, secret loss of 1.0277\n",
      "Training: Batch 4/250. Loss of 4.3202, cover loss of 0.0114, secret loss of 4.3087\n",
      "Training: Batch 5/250. Loss of 0.6803, cover loss of 0.0215, secret loss of 0.6588\n",
      "Training: Batch 6/250. Loss of 1.5741, cover loss of 0.0371, secret loss of 1.5370\n",
      "Training: Batch 7/250. Loss of 0.5574, cover loss of 0.0040, secret loss of 0.5533\n",
      "Training: Batch 8/250. Loss of 1.5127, cover loss of 0.0282, secret loss of 1.4845\n",
      "Training: Batch 9/250. Loss of 1.1470, cover loss of 0.0269, secret loss of 1.1201\n",
      "Training: Batch 10/250. Loss of 1.2875, cover loss of 0.0072, secret loss of 1.2803\n",
      "Training: Batch 11/250. Loss of 0.6675, cover loss of 0.0135, secret loss of 0.6540\n",
      "Training: Batch 12/250. Loss of 1.5753, cover loss of 0.0259, secret loss of 1.5494\n",
      "Training: Batch 13/250. Loss of 0.9491, cover loss of 0.0064, secret loss of 0.9428\n",
      "Training: Batch 14/250. Loss of 1.3012, cover loss of 0.0172, secret loss of 1.2840\n",
      "Training: Batch 15/250. Loss of 1.1978, cover loss of 0.0197, secret loss of 1.1781\n",
      "Training: Batch 16/250. Loss of 1.2658, cover loss of 0.0236, secret loss of 1.2422\n",
      "Training: Batch 17/250. Loss of 0.8820, cover loss of 0.0382, secret loss of 0.8438\n",
      "Training: Batch 18/250. Loss of 1.3520, cover loss of 0.0090, secret loss of 1.3430\n",
      "Training: Batch 19/250. Loss of 1.5610, cover loss of 0.0103, secret loss of 1.5507\n",
      "Training: Batch 20/250. Loss of 0.7695, cover loss of 0.0188, secret loss of 0.7507\n",
      "Training: Batch 21/250. Loss of 0.8930, cover loss of 0.0142, secret loss of 0.8788\n",
      "Training: Batch 22/250. Loss of 2.4847, cover loss of 0.0754, secret loss of 2.4093\n",
      "Training: Batch 23/250. Loss of 1.3863, cover loss of 0.0070, secret loss of 1.3793\n",
      "Training: Batch 24/250. Loss of 0.6795, cover loss of 0.0318, secret loss of 0.6477\n",
      "Training: Batch 25/250. Loss of 1.1897, cover loss of 0.0157, secret loss of 1.1740\n",
      "Training: Batch 26/250. Loss of 0.5085, cover loss of 0.0132, secret loss of 0.4953\n",
      "Training: Batch 27/250. Loss of 0.7631, cover loss of 0.0336, secret loss of 0.7295\n",
      "Training: Batch 28/250. Loss of 0.8504, cover loss of 0.0207, secret loss of 0.8296\n",
      "Training: Batch 29/250. Loss of 0.6468, cover loss of 0.0181, secret loss of 0.6287\n",
      "Training: Batch 30/250. Loss of 0.6098, cover loss of 0.0177, secret loss of 0.5921\n",
      "Training: Batch 31/250. Loss of 0.9093, cover loss of 0.0045, secret loss of 0.9048\n",
      "Training: Batch 32/250. Loss of 1.4704, cover loss of 0.0242, secret loss of 1.4462\n",
      "Training: Batch 33/250. Loss of 0.8521, cover loss of 0.0271, secret loss of 0.8250\n",
      "Training: Batch 34/250. Loss of 0.5054, cover loss of 0.0517, secret loss of 0.4537\n",
      "Training: Batch 35/250. Loss of 1.0286, cover loss of 0.0136, secret loss of 1.0150\n",
      "Training: Batch 36/250. Loss of 1.5727, cover loss of 0.0124, secret loss of 1.5603\n",
      "Training: Batch 37/250. Loss of 1.0961, cover loss of 0.0114, secret loss of 1.0848\n",
      "Training: Batch 38/250. Loss of 3.1668, cover loss of 0.0110, secret loss of 3.1558\n",
      "Training: Batch 39/250. Loss of 0.7358, cover loss of 0.0136, secret loss of 0.7222\n",
      "Training: Batch 40/250. Loss of 4.9190, cover loss of 0.0095, secret loss of 4.9095\n",
      "Training: Batch 41/250. Loss of 1.4287, cover loss of 0.0270, secret loss of 1.4017\n",
      "Training: Batch 42/250. Loss of 0.7954, cover loss of 0.0347, secret loss of 0.7607\n",
      "Training: Batch 43/250. Loss of 1.4174, cover loss of 0.0104, secret loss of 1.4070\n",
      "Training: Batch 44/250. Loss of 0.4749, cover loss of 0.0195, secret loss of 0.4554\n",
      "Training: Batch 45/250. Loss of 1.7603, cover loss of 0.0308, secret loss of 1.7295\n",
      "Training: Batch 46/250. Loss of 1.2712, cover loss of 0.0103, secret loss of 1.2609\n",
      "Training: Batch 47/250. Loss of 1.7580, cover loss of 0.0286, secret loss of 1.7294\n",
      "Training: Batch 48/250. Loss of 0.9577, cover loss of 0.0197, secret loss of 0.9380\n",
      "Training: Batch 49/250. Loss of 1.2646, cover loss of 0.0216, secret loss of 1.2429\n",
      "Training: Batch 50/250. Loss of 1.0021, cover loss of 0.0133, secret loss of 0.9888\n",
      "Training: Batch 51/250. Loss of 0.9805, cover loss of 0.0107, secret loss of 0.9698\n",
      "Training: Batch 52/250. Loss of 0.4361, cover loss of 0.0128, secret loss of 0.4233\n",
      "Training: Batch 53/250. Loss of 1.0028, cover loss of 0.0169, secret loss of 0.9859\n",
      "Training: Batch 54/250. Loss of 1.0143, cover loss of 0.0175, secret loss of 0.9968\n",
      "Training: Batch 55/250. Loss of 0.6918, cover loss of 0.0074, secret loss of 0.6845\n",
      "Training: Batch 56/250. Loss of 1.6500, cover loss of 0.0135, secret loss of 1.6365\n",
      "Training: Batch 57/250. Loss of 2.8544, cover loss of 0.0164, secret loss of 2.8381\n",
      "Training: Batch 58/250. Loss of 1.2622, cover loss of 0.0386, secret loss of 1.2237\n",
      "Training: Batch 59/250. Loss of 0.5520, cover loss of 0.0219, secret loss of 0.5300\n",
      "Training: Batch 60/250. Loss of 1.4071, cover loss of 0.0188, secret loss of 1.3884\n",
      "Training: Batch 61/250. Loss of 0.7887, cover loss of 0.0065, secret loss of 0.7822\n",
      "Training: Batch 62/250. Loss of 1.4085, cover loss of 0.0116, secret loss of 1.3968\n",
      "Training: Batch 63/250. Loss of 0.8714, cover loss of 0.0055, secret loss of 0.8658\n",
      "Training: Batch 64/250. Loss of 0.9245, cover loss of 0.0327, secret loss of 0.8918\n",
      "Training: Batch 65/250. Loss of 1.5716, cover loss of 0.0186, secret loss of 1.5531\n",
      "Training: Batch 66/250. Loss of 0.8079, cover loss of 0.0191, secret loss of 0.7888\n",
      "Training: Batch 67/250. Loss of 1.0197, cover loss of 0.0238, secret loss of 0.9959\n",
      "Training: Batch 68/250. Loss of 0.7713, cover loss of 0.0126, secret loss of 0.7586\n",
      "Training: Batch 69/250. Loss of 0.9606, cover loss of 0.0161, secret loss of 0.9445\n",
      "Training: Batch 70/250. Loss of 2.1839, cover loss of 0.0079, secret loss of 2.1760\n",
      "Training: Batch 71/250. Loss of 0.5568, cover loss of 0.0114, secret loss of 0.5454\n",
      "Training: Batch 72/250. Loss of 1.4307, cover loss of 0.0048, secret loss of 1.4259\n",
      "Training: Batch 73/250. Loss of 1.1981, cover loss of 0.0313, secret loss of 1.1668\n",
      "Training: Batch 74/250. Loss of 3.4709, cover loss of 0.0573, secret loss of 3.4136\n",
      "Training: Batch 75/250. Loss of 1.5156, cover loss of 0.0311, secret loss of 1.4845\n",
      "Training: Batch 76/250. Loss of 0.9707, cover loss of 0.0128, secret loss of 0.9579\n",
      "Training: Batch 77/250. Loss of 1.1370, cover loss of 0.0344, secret loss of 1.1026\n",
      "Training: Batch 78/250. Loss of 1.1422, cover loss of 0.0225, secret loss of 1.1198\n",
      "Training: Batch 79/250. Loss of 1.8856, cover loss of 0.0193, secret loss of 1.8663\n",
      "Training: Batch 80/250. Loss of 1.1170, cover loss of 0.0321, secret loss of 1.0849\n",
      "Training: Batch 81/250. Loss of 0.9537, cover loss of 0.0121, secret loss of 0.9416\n",
      "Training: Batch 82/250. Loss of 1.3797, cover loss of 0.0143, secret loss of 1.3654\n",
      "Training: Batch 83/250. Loss of 1.7820, cover loss of 0.0315, secret loss of 1.7504\n",
      "Training: Batch 84/250. Loss of 0.9847, cover loss of 0.0220, secret loss of 0.9627\n",
      "Training: Batch 85/250. Loss of 0.4974, cover loss of 0.0084, secret loss of 0.4890\n",
      "Training: Batch 86/250. Loss of 1.7232, cover loss of 0.0177, secret loss of 1.7054\n",
      "Training: Batch 87/250. Loss of 2.8927, cover loss of 0.0139, secret loss of 2.8788\n",
      "Training: Batch 88/250. Loss of 0.3459, cover loss of 0.0556, secret loss of 0.2902\n",
      "Training: Batch 89/250. Loss of 0.6005, cover loss of 0.0139, secret loss of 0.5866\n",
      "Training: Batch 90/250. Loss of 1.1071, cover loss of 0.0097, secret loss of 1.0973\n",
      "Training: Batch 91/250. Loss of 0.7674, cover loss of 0.0190, secret loss of 0.7484\n",
      "Training: Batch 92/250. Loss of 2.0656, cover loss of 0.0165, secret loss of 2.0491\n",
      "Training: Batch 93/250. Loss of 0.6362, cover loss of 0.0246, secret loss of 0.6116\n",
      "Training: Batch 94/250. Loss of 1.1124, cover loss of 0.0272, secret loss of 1.0852\n",
      "Training: Batch 95/250. Loss of 5.0261, cover loss of 0.0136, secret loss of 5.0125\n",
      "Training: Batch 96/250. Loss of 1.1928, cover loss of 0.0154, secret loss of 1.1775\n",
      "Training: Batch 97/250. Loss of 1.4735, cover loss of 0.0099, secret loss of 1.4636\n",
      "Training: Batch 98/250. Loss of 1.4287, cover loss of 0.0218, secret loss of 1.4068\n",
      "Training: Batch 99/250. Loss of 1.1238, cover loss of 0.0165, secret loss of 1.1072\n",
      "Training: Batch 100/250. Loss of 3.9580, cover loss of 0.0165, secret loss of 3.9414\n",
      "Training: Batch 101/250. Loss of 1.6290, cover loss of 0.0215, secret loss of 1.6075\n",
      "Training: Batch 102/250. Loss of 1.1875, cover loss of 0.0126, secret loss of 1.1750\n",
      "Training: Batch 103/250. Loss of 1.1823, cover loss of 0.0267, secret loss of 1.1556\n",
      "Training: Batch 104/250. Loss of 1.3503, cover loss of 0.0155, secret loss of 1.3348\n",
      "Training: Batch 105/250. Loss of 1.7498, cover loss of 0.0194, secret loss of 1.7304\n",
      "Training: Batch 106/250. Loss of 1.9362, cover loss of 0.0100, secret loss of 1.9263\n",
      "Training: Batch 107/250. Loss of 2.2386, cover loss of 0.0099, secret loss of 2.2287\n",
      "Training: Batch 108/250. Loss of 1.3584, cover loss of 0.0067, secret loss of 1.3517\n",
      "Training: Batch 109/250. Loss of 1.4741, cover loss of 0.0076, secret loss of 1.4665\n",
      "Training: Batch 110/250. Loss of 2.0723, cover loss of 0.0300, secret loss of 2.0423\n",
      "Training: Batch 111/250. Loss of 1.0065, cover loss of 0.0076, secret loss of 0.9989\n",
      "Training: Batch 112/250. Loss of 1.3863, cover loss of 0.0224, secret loss of 1.3639\n",
      "Training: Batch 113/250. Loss of 1.0085, cover loss of 0.0154, secret loss of 0.9931\n",
      "Training: Batch 114/250. Loss of 3.8142, cover loss of 0.0062, secret loss of 3.8081\n",
      "Training: Batch 115/250. Loss of 1.0790, cover loss of 0.0140, secret loss of 1.0651\n",
      "Training: Batch 116/250. Loss of 4.5462, cover loss of 0.0093, secret loss of 4.5369\n",
      "Training: Batch 117/250. Loss of 2.1553, cover loss of 0.0178, secret loss of 2.1374\n",
      "Training: Batch 118/250. Loss of 2.1366, cover loss of 0.0128, secret loss of 2.1238\n",
      "Training: Batch 119/250. Loss of 1.1631, cover loss of 0.0292, secret loss of 1.1339\n",
      "Training: Batch 120/250. Loss of 1.2387, cover loss of 0.0092, secret loss of 1.2295\n",
      "Training: Batch 121/250. Loss of 1.1152, cover loss of 0.0075, secret loss of 1.1077\n",
      "Training: Batch 122/250. Loss of 1.7054, cover loss of 0.0170, secret loss of 1.6884\n",
      "Training: Batch 123/250. Loss of 1.9565, cover loss of 0.0093, secret loss of 1.9472\n",
      "Training: Batch 124/250. Loss of 1.0443, cover loss of 0.0122, secret loss of 1.0321\n",
      "Training: Batch 125/250. Loss of 1.6781, cover loss of 0.0156, secret loss of 1.6626\n",
      "Training: Batch 126/250. Loss of 0.7546, cover loss of 0.0290, secret loss of 0.7255\n",
      "Training: Batch 127/250. Loss of 1.3451, cover loss of 0.0057, secret loss of 1.3394\n",
      "Training: Batch 128/250. Loss of 0.8112, cover loss of 0.0077, secret loss of 0.8035\n",
      "Training: Batch 129/250. Loss of 0.5812, cover loss of 0.0089, secret loss of 0.5723\n",
      "Training: Batch 130/250. Loss of 1.7214, cover loss of 0.0182, secret loss of 1.7032\n",
      "Training: Batch 131/250. Loss of 3.9477, cover loss of 0.0151, secret loss of 3.9326\n",
      "Training: Batch 132/250. Loss of 1.3590, cover loss of 0.0078, secret loss of 1.3512\n",
      "Training: Batch 133/250. Loss of 1.3727, cover loss of 0.0084, secret loss of 1.3643\n",
      "Training: Batch 134/250. Loss of 1.5135, cover loss of 0.0072, secret loss of 1.5063\n",
      "Training: Batch 135/250. Loss of 1.1851, cover loss of 0.0103, secret loss of 1.1747\n",
      "Training: Batch 136/250. Loss of 1.5498, cover loss of 0.0168, secret loss of 1.5330\n",
      "Training: Batch 137/250. Loss of 0.5583, cover loss of 0.0068, secret loss of 0.5515\n",
      "Training: Batch 138/250. Loss of 1.0481, cover loss of 0.0208, secret loss of 1.0273\n",
      "Training: Batch 139/250. Loss of 1.7176, cover loss of 0.0199, secret loss of 1.6977\n",
      "Training: Batch 140/250. Loss of 0.9310, cover loss of 0.0089, secret loss of 0.9222\n",
      "Training: Batch 141/250. Loss of 1.1572, cover loss of 0.0114, secret loss of 1.1459\n",
      "Training: Batch 142/250. Loss of 2.8824, cover loss of 0.0090, secret loss of 2.8734\n",
      "Training: Batch 143/250. Loss of 0.1603, cover loss of 0.0161, secret loss of 0.1443\n",
      "Training: Batch 144/250. Loss of 2.8547, cover loss of 0.0109, secret loss of 2.8438\n",
      "Training: Batch 145/250. Loss of 1.2443, cover loss of 0.0115, secret loss of 1.2328\n",
      "Training: Batch 146/250. Loss of 0.8874, cover loss of 0.0112, secret loss of 0.8763\n",
      "Training: Batch 147/250. Loss of 0.7260, cover loss of 0.0356, secret loss of 0.6904\n",
      "Training: Batch 148/250. Loss of 0.6740, cover loss of 0.0160, secret loss of 0.6580\n",
      "Training: Batch 149/250. Loss of 1.1633, cover loss of 0.0161, secret loss of 1.1471\n",
      "Training: Batch 150/250. Loss of 0.5768, cover loss of 0.0198, secret loss of 0.5570\n",
      "Training: Batch 151/250. Loss of 0.6539, cover loss of 0.0144, secret loss of 0.6395\n",
      "Training: Batch 152/250. Loss of 1.0588, cover loss of 0.0184, secret loss of 1.0404\n",
      "Training: Batch 153/250. Loss of 1.9105, cover loss of 0.0136, secret loss of 1.8969\n",
      "Training: Batch 154/250. Loss of 1.4918, cover loss of 0.0113, secret loss of 1.4805\n",
      "Training: Batch 155/250. Loss of 1.5073, cover loss of 0.0058, secret loss of 1.5015\n",
      "Training: Batch 156/250. Loss of 1.0396, cover loss of 0.0128, secret loss of 1.0268\n",
      "Training: Batch 157/250. Loss of 1.5708, cover loss of 0.0087, secret loss of 1.5621\n",
      "Training: Batch 158/250. Loss of 1.1382, cover loss of 0.0072, secret loss of 1.1310\n",
      "Training: Batch 159/250. Loss of 0.6148, cover loss of 0.0074, secret loss of 0.6074\n",
      "Training: Batch 160/250. Loss of 1.1547, cover loss of 0.0093, secret loss of 1.1454\n",
      "Training: Batch 161/250. Loss of 0.6564, cover loss of 0.0132, secret loss of 0.6433\n",
      "Training: Batch 162/250. Loss of 0.2481, cover loss of 0.0188, secret loss of 0.2293\n",
      "Training: Batch 163/250. Loss of 0.5243, cover loss of 0.0125, secret loss of 0.5119\n",
      "Training: Batch 164/250. Loss of 1.1447, cover loss of 0.0068, secret loss of 1.1379\n",
      "Training: Batch 165/250. Loss of 0.7325, cover loss of 0.0071, secret loss of 0.7254\n",
      "Training: Batch 166/250. Loss of 1.2391, cover loss of 0.0201, secret loss of 1.2190\n",
      "Training: Batch 167/250. Loss of 1.3112, cover loss of 0.0091, secret loss of 1.3021\n",
      "Training: Batch 168/250. Loss of 1.6563, cover loss of 0.0133, secret loss of 1.6430\n",
      "Training: Batch 169/250. Loss of 1.0835, cover loss of 0.0134, secret loss of 1.0700\n",
      "Training: Batch 170/250. Loss of 0.3605, cover loss of 0.0165, secret loss of 0.3439\n",
      "Training: Batch 171/250. Loss of 2.0100, cover loss of 0.0098, secret loss of 2.0002\n",
      "Training: Batch 172/250. Loss of 1.2812, cover loss of 0.0143, secret loss of 1.2669\n",
      "Training: Batch 173/250. Loss of 0.8418, cover loss of 0.0151, secret loss of 0.8266\n",
      "Training: Batch 174/250. Loss of 0.7568, cover loss of 0.0310, secret loss of 0.7258\n",
      "Training: Batch 175/250. Loss of 1.7013, cover loss of 0.0178, secret loss of 1.6835\n",
      "Training: Batch 176/250. Loss of 0.3961, cover loss of 0.0064, secret loss of 0.3896\n",
      "Training: Batch 177/250. Loss of 4.0911, cover loss of 0.0245, secret loss of 4.0666\n",
      "Training: Batch 178/250. Loss of 1.4179, cover loss of 0.0093, secret loss of 1.4087\n",
      "Training: Batch 179/250. Loss of 1.0032, cover loss of 0.0332, secret loss of 0.9700\n",
      "Training: Batch 180/250. Loss of 1.1397, cover loss of 0.0049, secret loss of 1.1348\n",
      "Training: Batch 181/250. Loss of 0.7984, cover loss of 0.0085, secret loss of 0.7899\n",
      "Training: Batch 182/250. Loss of 0.7147, cover loss of 0.0071, secret loss of 0.7076\n",
      "Training: Batch 183/250. Loss of 1.3849, cover loss of 0.0085, secret loss of 1.3765\n",
      "Training: Batch 184/250. Loss of 1.9821, cover loss of 0.0147, secret loss of 1.9674\n",
      "Training: Batch 185/250. Loss of 2.2061, cover loss of 0.0127, secret loss of 2.1934\n",
      "Training: Batch 186/250. Loss of 1.7695, cover loss of 0.0119, secret loss of 1.7576\n",
      "Training: Batch 187/250. Loss of 1.4860, cover loss of 0.0079, secret loss of 1.4780\n",
      "Training: Batch 188/250. Loss of 1.1154, cover loss of 0.0185, secret loss of 1.0969\n",
      "Training: Batch 189/250. Loss of 1.0114, cover loss of 0.0059, secret loss of 1.0054\n",
      "Training: Batch 190/250. Loss of 0.8647, cover loss of 0.0075, secret loss of 0.8572\n",
      "Training: Batch 191/250. Loss of 1.0610, cover loss of 0.0065, secret loss of 1.0545\n",
      "Training: Batch 192/250. Loss of 1.1246, cover loss of 0.0086, secret loss of 1.1160\n",
      "Training: Batch 193/250. Loss of 2.9473, cover loss of 0.0137, secret loss of 2.9336\n",
      "Training: Batch 194/250. Loss of 1.3029, cover loss of 0.0144, secret loss of 1.2885\n",
      "Training: Batch 195/250. Loss of 0.4254, cover loss of 0.0143, secret loss of 0.4111\n",
      "Training: Batch 196/250. Loss of 1.4810, cover loss of 0.0104, secret loss of 1.4706\n",
      "Training: Batch 197/250. Loss of 0.9100, cover loss of 0.0069, secret loss of 0.9031\n",
      "Training: Batch 198/250. Loss of 1.2587, cover loss of 0.0076, secret loss of 1.2511\n",
      "Training: Batch 199/250. Loss of 1.9925, cover loss of 0.0158, secret loss of 1.9767\n",
      "Training: Batch 200/250. Loss of 1.2426, cover loss of 0.0121, secret loss of 1.2305\n",
      "Training: Batch 201/250. Loss of 1.4426, cover loss of 0.0076, secret loss of 1.4351\n",
      "Training: Batch 202/250. Loss of 1.0574, cover loss of 0.0168, secret loss of 1.0406\n",
      "Training: Batch 203/250. Loss of 2.9227, cover loss of 0.0076, secret loss of 2.9151\n",
      "Training: Batch 204/250. Loss of 0.8002, cover loss of 0.0031, secret loss of 0.7971\n",
      "Training: Batch 205/250. Loss of 0.6891, cover loss of 0.0081, secret loss of 0.6811\n",
      "Training: Batch 206/250. Loss of 1.6278, cover loss of 0.0051, secret loss of 1.6228\n",
      "Training: Batch 207/250. Loss of 1.1877, cover loss of 0.0077, secret loss of 1.1799\n",
      "Training: Batch 208/250. Loss of 0.5162, cover loss of 0.0181, secret loss of 0.4981\n",
      "Training: Batch 209/250. Loss of 1.3937, cover loss of 0.0078, secret loss of 1.3858\n",
      "Training: Batch 210/250. Loss of 1.2907, cover loss of 0.0246, secret loss of 1.2661\n",
      "Training: Batch 211/250. Loss of 2.0448, cover loss of 0.0384, secret loss of 2.0064\n",
      "Training: Batch 212/250. Loss of 0.8839, cover loss of 0.0082, secret loss of 0.8757\n",
      "Training: Batch 213/250. Loss of 0.6962, cover loss of 0.0083, secret loss of 0.6880\n",
      "Training: Batch 214/250. Loss of 0.9199, cover loss of 0.0068, secret loss of 0.9131\n",
      "Training: Batch 215/250. Loss of 0.9393, cover loss of 0.0123, secret loss of 0.9270\n",
      "Training: Batch 216/250. Loss of 0.4376, cover loss of 0.0059, secret loss of 0.4317\n",
      "Training: Batch 217/250. Loss of 1.7378, cover loss of 0.0045, secret loss of 1.7333\n",
      "Training: Batch 218/250. Loss of 1.1238, cover loss of 0.0126, secret loss of 1.1112\n",
      "Training: Batch 219/250. Loss of 1.6486, cover loss of 0.0102, secret loss of 1.6384\n",
      "Training: Batch 220/250. Loss of 3.2139, cover loss of 0.0098, secret loss of 3.2042\n",
      "Training: Batch 221/250. Loss of 0.9425, cover loss of 0.0067, secret loss of 0.9358\n",
      "Training: Batch 222/250. Loss of 0.7673, cover loss of 0.0068, secret loss of 0.7606\n",
      "Training: Batch 223/250. Loss of 0.6475, cover loss of 0.0085, secret loss of 0.6391\n",
      "Training: Batch 224/250. Loss of 1.4577, cover loss of 0.0132, secret loss of 1.4445\n",
      "Training: Batch 225/250. Loss of 2.4509, cover loss of 0.0062, secret loss of 2.4447\n",
      "Training: Batch 226/250. Loss of 0.6547, cover loss of 0.0078, secret loss of 0.6469\n",
      "Training: Batch 227/250. Loss of 0.8969, cover loss of 0.0070, secret loss of 0.8900\n",
      "Training: Batch 228/250. Loss of 0.9501, cover loss of 0.0243, secret loss of 0.9257\n",
      "Training: Batch 229/250. Loss of 1.8587, cover loss of 0.0064, secret loss of 1.8523\n",
      "Training: Batch 230/250. Loss of 2.4702, cover loss of 0.0044, secret loss of 2.4657\n",
      "Training: Batch 231/250. Loss of 2.0483, cover loss of 0.0059, secret loss of 2.0424\n",
      "Training: Batch 232/250. Loss of 0.9348, cover loss of 0.0087, secret loss of 0.9261\n",
      "Training: Batch 233/250. Loss of 0.8877, cover loss of 0.0075, secret loss of 0.8802\n",
      "Training: Batch 234/250. Loss of 1.0717, cover loss of 0.0022, secret loss of 1.0695\n",
      "Training: Batch 235/250. Loss of 0.6235, cover loss of 0.0041, secret loss of 0.6195\n",
      "Training: Batch 236/250. Loss of 0.8504, cover loss of 0.0035, secret loss of 0.8470\n",
      "Training: Batch 237/250. Loss of 1.2949, cover loss of 0.0097, secret loss of 1.2852\n",
      "Training: Batch 238/250. Loss of 0.5408, cover loss of 0.0097, secret loss of 0.5311\n",
      "Training: Batch 239/250. Loss of 0.7175, cover loss of 0.0078, secret loss of 0.7097\n",
      "Training: Batch 240/250. Loss of 0.4098, cover loss of 0.0111, secret loss of 0.3986\n",
      "Training: Batch 241/250. Loss of 0.3323, cover loss of 0.0146, secret loss of 0.3177\n",
      "Training: Batch 242/250. Loss of 0.9062, cover loss of 0.0059, secret loss of 0.9003\n",
      "Training: Batch 243/250. Loss of 0.6542, cover loss of 0.0101, secret loss of 0.6442\n",
      "Training: Batch 244/250. Loss of 0.7914, cover loss of 0.0094, secret loss of 0.7820\n",
      "Training: Batch 245/250. Loss of 4.8799, cover loss of 0.0077, secret loss of 4.8722\n",
      "Training: Batch 246/250. Loss of 1.3096, cover loss of 0.0093, secret loss of 1.3003\n",
      "Training: Batch 247/250. Loss of 0.6414, cover loss of 0.0041, secret loss of 0.6373\n",
      "Training: Batch 248/250. Loss of 1.5629, cover loss of 0.0084, secret loss of 1.5545\n",
      "Training: Batch 249/250. Loss of 1.5871, cover loss of 0.0123, secret loss of 1.5749\n",
      "Training: Batch 250/250. Loss of 2.0713, cover loss of 0.0041, secret loss of 2.0672\n",
      "Epoch [3/3], Average_loss: 1.3548\n"
     ]
    }
   ],
   "source": [
    "net, mean_train_loss, loss_history = train_model(train_loader, beta, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZgcVdX/v6d7ZjLZ10kgCxkgYQkhCRDDEhCBIKsgiqwCIsgPX1zh1RdeROVFlFcFAV9QNsWwKiiihH0PS4AEEgjZExISsq8zWWbrPr8/qm51LbeqblVXdff03M/zzDPdVbfuPV1169Spc889l5gZGo1Go6k+MuUWQKPRaDTpoBW8RqPRVClawWs0Gk2VohW8RqPRVClawWs0Gk2VohW8RqPRVClawWu6NETUSERMRDUKZb9BRG8UW49GUyq0gtd0GohoORG1EdEg1/bZpnJtLI9kGk1lohW8prPxCYBzxRciOhBA9/KJo9FULlrBazobDwC40Pb9IgBT7QWIqC8RTSWiDUS0goh+QkQZc1+WiH5LRBuJaBmAUyTH3kdEa4joMyL6BRFlowpJREOJ6F9EtJmIlhDRt2z7JhHRTCJqIqJ1RHSLub2eiB4kok1EtJWI3iOiIVHb1mgEWsFrOhszAPQhov1NxXs2gAddZX4PoC+AvQAcDeOBcLG571sATgVwEICJAM50HfsXAB0ARpllvgjg0hhyPgJgFYChZhu/JKLjzH23AbiNmfsA2BvA38ztF5lyjwAwEMDlAHbFaFujAaAVvKZzIqz44wEsAPCZ2GFT+tcwczMzLwdwM4ALzCJnAbiVmVcy82YAv7IdOwTASQB+wMw7mHk9gN8BOCeKcEQ0AsCRAP6LmVuYeTaAe20ytAMYRUSDmHk7M8+wbR8IYBQz55h5FjM3RWlbo7GjFbymM/IAgPMAfAMu9wyAQQDqAKywbVsBYJj5eSiAla59gpEAagGsMV0kWwHcBWBwRPmGAtjMzM0+MlwCYB8AC0w3zKm23/UcgEeJaDUR/ZqIaiO2rdFYaAWv6XQw8woYg60nA/iHa/dGGJbwSNu2PVCw8tfAcIHY9wlWAmgFMIiZ+5l/fZj5gIgirgYwgIh6y2Rg5sXMfC6MB8f/AniciHoyczszX8/MYwAcAcOVdCE0mphoBa/prFwC4Fhm3mHfyMw5GD7tG4moNxGNBHAlCn76vwH4HhENJ6L+AK62HbsGwPMAbiaiPkSUIaK9iejoKIIx80oAbwH4lTlwOs6U9yEAIKKvE1EDM+cBbDUPyxHRMUR0oOlmaoLxoMpFaVujsaMVvKZTwsxLmXmmz+7vAtgBYBmANwA8DOBP5r57YLhB5gB4H943gAthuHjmAdgC4HEAu8cQ8VwAjTCs+ScA/IyZXzD3nQjgYyLaDmPA9RxmbgGwm9leE4D5AF6DdwBZo1GG9IIfGo1GU51oC16j0WiqFK3gNRqNpkrRCl6j0WiqFK3gNRqNpkqpqNSmgwYN4sbGxnKLodFoNJ2GWbNmbWTmBtm+ilLwjY2NmDnTL/JNo9FoNG6IaIXfPu2i0Wg0mipFK3iNRqOpUrSC12g0mipFK3iNRqOpUrSC12g0mipFK3iNRqOpUrSC12g0mipFK3gAT87+DM0t7eUWQ6PRaBKlyyv4+Wua8P1HZ+PHj39YblE0Go0mUbq8gt/VbiyYs2ZbS5kl0Wg0mmTp8gpeoJc90Wg01UaXV/AkPuiVrTQaTZWhFTxReCGNRqPphGgFb/7X9rtGo6k2tII3Nbz20Gg0mmojVQVPRP2I6HEiWkBE84no8DTbKwbWNrxGo6ky0l7w4zYAzzLzmURUB6BHyu1FhqB98BqNpjpJTcETUR8AnwfwDQBg5jYAbWm1VyzaRaPRaKqNNF00ewHYAODPRPQBEd1LRD3dhYjoMiKaSUQzN2zYkKI4crQPXqPRVCtpKvgaAAcD+AMzHwRgB4Cr3YWY+W5mnsjMExsapOvGajQajSYGaSr4VQBWMfM75vfHYSj8isKy4Msrhkaj0SROagqemdcCWElE+5qbjgMwL6324iIGWVn7aDQaTZWRdhTNdwE8ZEbQLANwccrtaTQajcYkVQXPzLMBTEyzjWLRg6wajaZa0TNZdRi8RqOpUrq8ghckMZN1wdomrNy8MwFpNBqNpnjS9sFXPIVB1uLrOvHW6QCA5TedUnxlGo1GUyRd3oLXYZKazsLmHW2Y9uGacouh6UR0eQWv0XQWLn9gFq54+H2sb9LLS2rU6PIKXrhmdBy8ptL5bOsuAEBbLl9mSTSdhS6v4AVavWs0mmqjyyt4nQdeo9FUK1rBC/2u9bxGo6kyuryCF2j9rtFoqo0ur+D1IKtGo6lWtILXtrtGo6lSuryCF2g1r9Foqo0ur+ALLpryylENTF+8AVt2VOyyuxpNl6PLK3hNMrS053DBfe/iG/e/V25RNBqNiVbwJtoXXxwdeeP8LVnXXGZJSs87yzZhyfqu97s1lU+XV/DaRaNOLs+YvXJrYJmueBrPvnsGptzyernF0Gg8aAVvqiSt4MO5/aXF+PIdb+KDT7eUWxSNRqNAl1fwGnXmr2kCAKxravUtoxfI0mgqhy6v4PVEpzjoc1UOdB/VREUr+HIL0IlQWb9Wn0+NpnLQCl5bRYmgXTPpQ3qFeE1EuryCF2g1Xxz6/KWPNkY0UUl10W0iWg6gGUAOQAczT0yzvThY2YL1vaOMPlcaTecgVQVvcgwzbyxBO7GwBlm1DRoKBThitHWZPtXsolm4thmL1jXjS+OHlluUqqIqXTSNV0/DFQ+/H+kYrZ+KQ5++9Knmh+gJt76O7z7yQbnFqDrSVvAM4HkimkVEl8kKENFlRDSTiGZu2LAhsYanfbgmgohaQakQ9JZTxbpHo+m0pK3gJzPzwQBOAnAFEX3eXYCZ72bmicw8saGhIWVxvGjFpNGo09KeQ0cuX24xNIqkquCZebX5fz2AJwBMSrO9YtCKPpwgH7x+Beoa7Hfds7jsgVnlFkOjSGoKnoh6ElFv8RnAFwHMTau9uLDkkyY6epA6fcQga7mNkZcXrC+vAFXC719ajAvueyfVNtKMohkC4AmzU9YAeJiZn02xvViU+2apNvT5TI9qHmTtitz8wqLU20hNwTPzMgDj06o/afS9o47sVOnzp9FUHlUZJhkFYRVp/RROUBg2K5TRFEeluGg0nQet4MX/Mt01s1durYpX72r4DZWOPseaqGgFX8Z75rmP1+LLd7yJv81cWT4hEkbroPTRA9oaVbq8gheU45ZZvnEHAGDphh1laD0+MiWuVU76aBeNJipVp+CjvsbqJfvUCfTB6/OXOtpFo4lK1Sn4fNR7oIz3jL5dNXHQ/UajStUp+FxkDW9QTuuoGgJPtF9Yo6k8qk7B5yO7aJz/y0FnU41SZd7ZfkQnpqu6ajbvaNN5cCKiFXwZNXxns9wD88GXUI6uTlc81ztaO3DwDS/gf56aV25ROhVVqODjHdcVbxqNprOwo7UDAPDM3LVllqRzUXUKPqoPvpy+42p6qHRRr0FZ6Mrnuiv/9jhUnYKPHCYpluzTg6xFoQdZ06ewZF8XPNfVcJOUgapT8LGjaBKWo6uiFX16dNXBVSf6HESh6hR8VP1eyEWTuChdCn3+SkdXPNeBi81ofKlCBR/VRdMF75YiCUpVoG9EjaZy0Are+l96Rd/pni2BqQo624/pvBR7pv/v5cU47JcvJSJLqRDDD7qbRSPNFZ3KQvyZrAkL0kXRPvj0Kbav/vb59FcSSpouPLxcFFVnwUfu/GXsMdW0OIZ+QGqSQL8JJkvVKfi4cfDFdquu1DGDfqn2wadPV3xLKvfCPJ2VqlPwUX3wFkX2m67Q71RUd1dUPqWmXH2tFMrVr4mucH+lQZdX8NZEpyIVU1fvf/oG1CSBXzfShkM8qlDBRyuflGKKY91Uk1LUN2DpKJ8FX4o2/Ex4xz+NIlWn4MsVRVPU4Z3MbS27CavpYVXplOthWopW/S349Glpz+HWFxehtSNXgtZKQ+oKnoiyRPQBET2VdltA+fLBdwUFR9UU9qOJTLX74O+dvgy3vrgYU99akX5jJaIUFvz3AcwvQTsAgHzE9QCS6rRd3UXRtX99aSmbi6Y8zZptp792ckt73vyvLXgliGg4gFMA3JtmO3biRtEUq+iLOrwKtKMOX9MkgZ+hpLtXPNK24G8F8GMAvnY1EV1GRDOJaOaGDRuKbjDXCZfsqyb0jVi9lOLabtvVjsXrmr1tWzLoDhaF1BQ8EZ0KYD0zzwoqx8x3M/NEZp7Y0NBQdLvx88EX224RB1eBa1vfdqWjfC6a9Bv+6h/ewvG/e93bdsCPbrx6Gn71TMm8wJ2KNC34yQBOI6LlAB4FcCwRPZhiewAAsSZvqccDu7wP3vz5ehy2einFg2Xl5l2x2r7rtWWJyVBNd3JqCp6Zr2Hm4czcCOAcAC8z89fTak8gfPDqeiahQdZq6hWxSOcErG9uwYvz1qVSd2elKxsTaf7yajROqi4OPm/GwWcUr1ZiE52SqabTk/SD7ty7Z+DSqTPRkYsYHlUiyuET7orGRCl/czWd35IoeGZ+lZlPLUVb+QBXwRl3volvP+gcEkjqWtpv9Marp2HOyq3hx3Syx0LQIzOtm+KTjTvSqTghqkkZhFHO3/qnNz8xhUivjSo04KvPghdRNLJJOR98uhXPzF1bEjkeefdT5bKlyMDY0p7DbIWHTlysFZ1S+imdQY82tbTj2blrUm+nXOeinAbJ/W8tN2XQRKHqFHxUH3xXcdH89Mm5+PIdb2LVlp2ptpOWlRc7S2jK2KW68q9zcPmD72PFpvhvHa8sXI+2jtK4o+JGnFU7ne3NOojqU/BRffAVMMiaVodavXUXFpkxxR+u2gYAaNrVUXS90jVZU7onCvHP6dRfLHYlKR6e21vjneN3lm3CxX9+Dzc/v1C5zWKo1HNaNso0yprmOE71KfhyhevFuEZpu2aOuOllfFESUxyXoHOa1kOqEpXQX95ajnE/fw6A87LXZI0TFDVdhmDzjjYAwIpNwW9ZiY0bpVw+DapxolOaP6nqFHyuyCia7z3yAe5+fWnkduMouHK8CqbmI0/5p1TSff2zf32MphbDSrfLlTVPbkdcDV/hlFK5qrZVDQo/zV9QdQqeo/rgXd//NWc1fvn0ghjtRj7EQljyO9s6sHZbS/yKqpjO4BfNZEwLPmWlU641DEp5BVRFS+NUl/qZoV00EbBy0ZR8JmvxnH3XDBz2q5cSqMlLkn1Ipmy7kgUvYGbHuagxFXyxIfvhD7OkMqBWLn6yubcn+RuEyij1edEWfASED17dRZPUgFXx9Xz02bYEJAmmGBdNYBx8yrdFJSojZpeLJtO5XDRRu2wpH7Kqb0FJvi2Vayar9sFHQETRlD4XTdehHNZ0Eg/Q1o4c3lu+OQFpDNwSZS0LvjhZwwbfkwvtjarhk2lXqSmfttzbK/HNLippGkdVo+AXrm3G/DVN1hNd1YJPijgdraTTrxPsRKUMkxQUqTMBANf/ex6+9se3sWT99uIrg+micVjwxu1UrIIPu1bJzb6OWL6EGl61rUqdHxEFbcErcMKtr+Ok26bbomjUjiubNWSjlM+iUsyaTYUErtO81U0AjJzjScBwXnczSjK2gq/GZFeAcT6u++dcrNysPsmurHq7Ch4aAiUFT0Q9iShjft6HiE4jotp0RYtHIW1taSc6FVNNaVarT7CulOuXt1l5vlb3by7WRVPqyJFS+eA/+HQLHpixAj/86+yi23L3g0R98J3V+AlA1YJ/HUA9EQ0D8BKAiwHcn5ZQxdCZUhV0VostyB+elp6vRKMq73HRlCZMshjsVnTUh2axvyrKeVEtW8GnWplKcNEQM+8E8BUAv2fmMwCMSU+s+Igwyc7gg++sSC34LhhFA7hmspo++I6UXTTFDDgf9etXYh9b0olOiuUq+WGqSiUMshIRHQ7gfADTzG016YhUHFFTFVSCD74UCOmKCpMUBwcMsqb1WE1CuSR9hYwwyUKtmYSiaELbTaqeyg2i8b3eniiaNNpOoc7A9irAgv8BgGsAPMHMHxPRXgDimwIpEj3ZWDJUehRNkgQ9zFJz0SRYV1IPIXadiWIHWUtNqaSMY1T4nUL3Zk5wyoFlv5T48qXZnJIVzsyvAXgNAMzB1o3M/L0U5YpNuV7Zimm1lN6kJM6PNEyy6Fqjt1meSvyryxbpoonbLgD89b1PceCwfhgztE+EeiL64EtqwqsWS3KQtTyUPVUBET1MRH2IqCeAeQAWEtGPUpOqCKwwScV3k0pMVpSmTElULY+iSVupJRlFk8ytzHDPZDX+51N30Xjr/6+/f4STb58esZ7i202rPV9DhN3lYotTMaT5E1RdNGOYuQnAlwE8DWAPABekJlURFHzBpXbRJHeZ0uy0xVjwVq6OcljwFVKHoz5mR6XCgm8vdqJTiZRW5HZiyxX9garaVCUaaB98usWac6FCJfjga8249y8DeJKZ21GhgQ2FKJrSthv1Ik19ezl+85x8YYc0Oq2oMxkLvvSXPslTkpwP3nkuRLKx1BcIVzwXbR153PnqEv8VoiryDjZQThecRttF1nrGnW9Fe5uqAAV/F4DlAHoCeJ2IRgJQf0SVECsO3vYavnrrLv8DytTJ73hlie++JCz4Urt5OsNEp6Rl9E02liuuoaTGZKa+vRy/fnZhYcFqF1HP6YV/ejcBqdTwH2RlV7kkXXeJVRWJsodJMvPtzDyMmU9mgxUAjklNqiJwJxt7bdEGHHHTy76LIZdryb4gF1ISndYdySG+JTLIGmFrUlTgm7iZLrhAxlrwI10XjWrtO1pz5n/5EoJRz+mCtc3RDiiiPeX7sgL7RVTK7qIhor5EdAsRzTT/boZhzVccVhy8+X2umYJ3zqp0U/FWWhy8n5IpqjMFOeFTJokWk7bQ3KdBLNmXtotG9fR31pnSgP9vdG9PY7yqEo2JuKi6aP4EoBnAWeZfE4A/Bx1ARPVE9C4RzSGij4no+uJEVSNnWfDxluyLS5KdIgkru91HyaQ1WJm6iyal8M6i6oNTLtHlchWmIRQDUioK5bw8ieYoKtOi2ynWrTobdW9m/qrt+/VEFJY5qBXAscy83RygfYOInmHmGbEkVSSKIujI5TF98cZk2o1YPqgvJWGVeNacYFF3Oooy9SiaJAdZE0s25lIvLLYXV2+YfKpKrbBCkd/bXGlUfMAEaF/U0wVHlye87dJS9jh4ALuI6EjxhYgmAwgYuQRMX71IvF1r/qV+7qJYT7e9tBjTPir45g/46bOx23VfpGKUSBIX3E+RJxJFI6kkfQs+gToS7n6GBe/8DhR//UJ98BFdNJ3RgleeyVphb0txqAQL/nIAU4mor/l9C4CLwg4ioiyAWQBGAbiDmd+RlLkMwGUAsMceeyiK44/oGCoXfukG58IPO9pysdv1drzg8uTzGUgoisZve/rhLilVm+CreEKBkkYYPNu+G5/jX79kXQTC5eDfFxJtLlGUwyQr+DeoUvZBVmaew8zjAYwDMI6ZDwJwrMJxOWaeAGA4gElENFZS5m5mnsjMExsaGiKK70X44FXOWZJLZyZ5kdKw4Nn1vxjkPvjKj6JJ3gfvOscs3x6lxjil4p77UgUGxHlsKfvgS6Tgv/3gLFzx0PvW97aOPFra4xuEdsoeJilg5iZzRisAXBnhuK0AXgVwYpT2VFnf3GJ9tiZ1KJyzZPPWJFdXmsosrbqtTSmNUyV5pRIbS2M4BBOWe1ozkaNY4u8s24TlG3f47g+ssEQ0t7T7Kkn//uvcUarcU8/MXetw5x7z21ex33XxXboOym3B+xB4mxBRAxH1Mz93BzAFwIIi2vPl1NvfsD6L6BElCz5RqztaefuIvfvQZAZC5XWkFwevsrOINhMcHE4scgrOnysssfjnWDXyS/52Zufsu2fgsVmrHHJ56okkW3z82jnw58/jmN++6nNMvLcZAHj+47U4/Y43Y+cEUrl8nwVNnozaXmI1eSlGwYfJtTuAV4joQwDvAXiBmZ8qoj1f1je3Wp8tBa9wlZJ0K7hrKsZKTNcHH79O4bsuyyBronUlU5t7RSfrc0rnohAV45UjkBTf5gS5PGPPa6bhwRkrJO34P1nXbGvxbAOKG2T9ziMfYM7KrZjyu9cCZXZTtpms5bLgiaiZiJokf80AhgYdy8wfMvNBzDyOmccy8/8kKrkP7Tl1H7xKmVkrNuPo37yC7T6zAa26OPh7EO5+lYQC8vjgrVw06fSm1Fd0SrD6pN7c/AdZ0zrH/nLEOS5JWtpzYAZunDbf234MAez91PnZWU56Lc1tyzbsiN4wSjc2UYr2AhU8M/dm5j6Sv97MXJErOrVZFnx4WZUb8bfPLcKKTTsx+9OtgeWSvEip+uCLr1ped+r3RJLnN5m63LW8umgDgBKksPUoueAGfVdHSvKcmv9lVnCcVuzHBJ/P5H5DuRbdLnsUTWeivUP44N0WrLesyo3Yt3stAGDbrvbActF98P77krAAU42DL8PoXKJRSknV43LRrNi009yeUAMuwiYu+ZHmgHuhLqMyWbfOR4hsc9fn/uypW7Kr6P5Z5sHnJKk+Be9jwcv9xuFXUij4ppZgBS9j5eadsazFNCx48bWofPABE2fcm56duwYvL1gXuy03SY5LJDbIynJdEP8cR1eEKu35unYithMogxm8JpvuH8uCt49tKJYrlvJlk0yPqlPwfi4aqVdB4cz26W54opoiWvDz1zThqF+/gvvfWi4tH9WCb8/lMfXt5cqJrFJ10Si0d/mD7+Ob989MoDXRZrLDrInVFNNwkNcVr1zcma9Jjsfkgiz4GO3YH+hBh8vq7myTnyohVUGnob1DfrJkIVMqHU/ZReNSGp+YMcjvLd8c2oZb2cvEmvr2Cvz0yY8x9W1vlEKYPPYOlFYumrRJ5q1GDDT7l/nW1Jm4/aXFyjKpvM2o4s6EGtSu87gwCz49d50gFyR8nEFWe/8NqCDJ818utA8+Am0+YZL3vvGJp6yKsutWkwWA6FE05v+nP1obOR5XJpbI6b15R5tSHW4LKMkQPtkN15miaIKqemHeOtzywiLFenyMibgWfEwXTVj5UjyQ84EWfPT6pOGnAe06j+1sKj49qk7BJz3RSbx6+qXffWXB+lD//BtLvBkr7SP2KhaZtVqQ4t3iGKSyby9CEQeuyZryPVVpUUpB9cQdL1B20bi/x0y5kYYFL/fBh785uSnmTbPYn1Xqx4O24CPgp4ilKJxY0dFaJetart66Cxff/x5++Ohsz0Wyd3PZsSpt2qnLGpdK2Qdv/2x30aS0FkXaN0WycfDJVMZI9mEXaon7bi/ujSEJCgpe0k6RFnzQ9Up0yT6r7dKq+IrJRVOp3HHewdZnYeEmFQcv3CvtknU2d5l5ND7ZuMNzkeyWjHv5vDBkpcVqQaoPMKffPf7rf1jdnn2dwAOa1P2bZ5a7q2IPskZ/OzPkiNVcogS7aKILqOqiqYooGm3BByPcF0BhwWMVRaPS8YQ+besIzhwX3Am9O92d6ZWF6wPL10Rw0TwwYwU2bi/46vPMWLl5l2/dUZFbrZ3JB5+QBe83yJqSi8YvVUHYufed6ORzWFvEN04gzEUTHecga1A5ybYKeOBFIU1xq0LB7zGgh/W5I8JMVjU/vVEqrNO767L387B2iIA7X1lia9NbpsZy0QTXtmhdM67751x895EPrG1/f39VYN1JIKpNazZgotE/iZ0D+aOi2EFW//1ywq6pv2vHy3vLN2OfnzyDt5ZGW+ksyIK3opcinHinBV8aF01avLJgPZ6du8Z3vw6TDGHM0D64+qT9AADtCrPmzvrj2/hk447AG+PZuWsBFDpQS7u/gjd8sc7KwnyI7hshQ/6DrkDBgm8PcaKLB5E92qa5xR4BlICiDNyXTmdNstak6irbIKurXOiDwWe3TLHMWLoJAPCmJDAgiJw10Um9/SDs90xgf0uhuyVV5ym3T8dp//cGLr7/PVz+4Pu+5bQFr8C+u/UG4LXgZREu7y7fjJufXxj45Lz8wVkACq+eLRIXjb0vB10kFR+8/caQPRCED17Vn2+vI+OoW+nwQKSnzbYt6piDWptJPJjUx2fU6pPLFfchF3ba/BJMexS+29jwi4OXtRGyzJ8fhWvu1fCxwiTtnyO6P+OS9Nvnx6ub8OGqbYnWGZWqUfBZs2cWXBiMHa0duOu1Zb7HKA2ymkVkFrw4urU9h6/c+ZZjn8NFo9AH7Ra8TK5aRReNrM2wtwNVCosnByu1SJFMiiRrwSfog/fZHq++eC4ab5itmjylikySTTCLMm4QPKhfWZQrNYkfVaPghRKzR9EExaczokXatEpWnhERNk4XiIHdFpC6aFzvsmFKWOwPU56yV2Tn4iIJ+rIl2whkTTZLkkQHWX3qmvahv59UWg9YWld8H7z5P8yn7lHozg3uNyiRBM2/xQJh67j6ERgmKSkfOm7A8s+eehJ8W0zr3lBpOS2qSMEb/zvyhYlOYa4CpYlOZqFdEgWfK2i1yO14fPCZYAUvbmLViU5+bRVzPwi5wnzw7TGiMBRaT7Wmz7buwhUP+/tJpfWwvLbYD6O4lr/ru1vhv7FkI7ZIZkAnGX4YlItGZtWG3Zvs8zmoXFIUU2exIaFJUz0KXgxCmi6MzTvaghfFZbXXqcIgq0TBq/rDVXzwkjadchj/47g/7Ba1ym+esWwTzvzDW/5tyfzOtk2yOQPFkrY7Ic5DyS9MMq4Fb0WihBgMHoXu6l+y5mWpNgJ98BHVXD7Igo9xjuy/qVRRNJYBU0SVxY43JE31KHhJz/rR4x/6lmdwpIlOMmVuBbSEVBO+IINzIFTu1/WXw45soOimZwpL4a7asgtrtgWvJ/mfj83BzBVbsGarczk1dv2X7QPixVGHkcRNEBQmKes/bi647x08NnOl9T3pnPthLpqgVBF2VJVekg9Ny0WjOMiqkuK4uaUd23a2B1/7JF13CdShLfiUyEp+yQchqzBFyUUjKyv25SRXiByDppKKXQo9bJDVctEUaR3/5rmFOPxXLweWyVh+WLllGNYh0/DBp+1rVZnFOH3xRo/RIH/YxfTBBxw297NtWO27fqnLB04Zv+kAACAASURBVC+pKJMhpbc3S0HHddFIffDRXTR5Zhxyw4sY/z/PB/vgk1Twrrq27WxXTg3iV4dzn49BoH3w4chm0AXBHG0mq6xDim1hce4q7TgGQl3lH5yxAm8tMeKT01CeXlmM/55oDCvdgcxFU9hWqVE0QRZyXMtLdthzH69D49XTsHWnWubPgnz+Mpz6+zd8y3nCJCWnn+B/PR3l4un3woIfru3zVjfho8+2eeQMVcxsywwbIE2yyw4W6srlGeP/53n85J9zI9UR1I/8clJpC14BlVdsDwonlgOsdHExwxJ4hT0APC4aV/Gf/HMuHptlzEYNc38kkU8jLOmSNIrG9jmdOPh064ojM0Oei0bUtWjd9kj1WSnVQ67hC/OcK2W5+5ffG2DQZDxB3IRbBQveKfzJt0+XhiqHvZHZ9xZjwUf5HYUgArbGLJ6cvVr5eEMereBTIRtRs81f04RlG8NXXRc3a1AkQEeIhg/vzOxy0fiXlU24Shohi0cOdvxz7rJtTEXBJxLCJt5AnMxYtgk726KfVz8L3t1epAoRfsO7lY67uFTB59Xi4+NOdIrqQpMZTI76bPvdRZesb7btC7m3YnabZjPEuq4mmooMUgWtPvdumi6amtRqLjFRLdflvrHBBZi54GeXDrL6++cd5RReRzO2fhRoBQSkTACSsQYKN7nL4lNoQ9X1pYIjtC9RC75Q2dIN23HO3TMwfnjf6PUUub/Y8tZxHgveWybH3qACqYsGycfBF9qzy6iumN1yrt7aglGDe3vqlJFnRkZxhqr9PAoLPrKCD/hdfm/fndKCJ6IRRPQKEc0noo+J6PtptQXEdNGEkLdZaMZn55UIs0IK9ch8nc6J504fvH9dflZAUFtREbK4f19w0qjCtrgytLTn8KLN/XDQDS9Iao8Pu/4DhaUY561pCj5WGhoqn+gkiHoeYkffuI4LGqQPay92qgLFEE9LHpuue3L2Z979NgGOu/k1Z1uKIZRGPWry2GEuTF6sk0VvBLYX3UXzm+cW4rT/e0O6r1jSdNF0ALiKmfcHcBiAK4hoTFqN2VMGJ0Uuz47OJD5u29mOxqun4R/vezumICi3zFtLN2LJeqd/NhMwyGpHljJhwdomPDt3DZ74YFUyFrz53zdiR+aDt/mP446x3vDUPFw6dSbmrPRGP5XbBy/bzQh+vY7qqvJTDmHjLu5m5C4atbBgQew4eEVr2W48fP/R2ZL2C7jdZ04FHyJXDB88UHDRdFOw4B+csQKNV09DS3su8IHi9/a9obkV65taleWMQmouGmZeA2CN+bmZiOYDGAZgXhrtpaDfDQXPdgXPyIKwaqvh3nniA38Fb8c9+/S8e95xfGdm5YRgsglXJ9463fr85BWTlWQKwp32ATA68YK1hu+TYZwb+0P12w8Zs0CZ4/vgxZR62QLnyS7Z51UQYSLLflOYDz7qfAC/uq594qPg41znRvZbVm3dZa0vHNSelaogdQs+5IEasN95Twa3o/I7bpw2D2ubWjF6cC9rm2XBuxR8Ps+YvcppgPz+ZWOR9i072wLHAv3ui7ZcPrIrSJWSDLISUSOAgwC8I9l3GRHNJKKZGzZsiN1GJgUN/+aSjdhsWzhDXKD62qzfIRb2a5kLiV03omjU8sWEpSooRg3OWrEFn27aad2kOdt79E/+OddS8C/OW4e9//tpLFzbLKsmtosmSDkkGu8c8M2//WC3lAyVpRqZGY1XT8MfX1vqW9srC4PvC4+LRnKyLv7ze/j8b15xHif1wcejMNFJjbA+IksNYm9LpMMOd9GEX997pn+Cf89Z7TiPwgdf63LR3D19mSexoPX2Cgrsp36ytHbkOq+CJ6JeAP4O4AfM7HF0MvPdzDyRmSc2NDTEbicNH/ylU2fi7WWbrO/iAqlYqPaLqaKUSdGCD6OYCJav/uEthxLwc9GI6COZK6VYGQC/KJ0kLXj55yD80kcUM14CFNI6/PrZBb6/sXtd8G1ql+2zrbuKcmf5DbCHUUiz4H8f2usM6yM7JKkVBDOXb8HBN7yAaR+uUQhwiOeWEu4Ut924QDJWI47KUHB7fmN2re15z4MkKVJV8ERUC0O5P8TM/0izrTRcNG5Ep1R59bZbUWFhlG4LPqxTBt0cUWfeyXC7aPxel/0myqgOPkchkRpZ/GP3plCkPviQg8Minox6C4rRXd9nW3fhqQ9Xo74m+I3Rftzkm17GzBWbQ9t1HyfwWxYwjJzPRCc/wvq4LEOrQPw+43/42Akz4+bnF2Le6rCB9MJncc+q9GXn+sf+5f3uo07poiHjUX4fgPnMfEta7QjSsODdREn4Zb+UKhkgHQ+okOJBr69JJPoSp1LI7dfJ/ZRekPX3ycYdOOyXL2Gtz7R7X1xVbmhuNdwaMR4mcSx4uQ9ePtFJoOKisfcNd11f+8Nb+M7DH6BbbfBt6v4NC9fJXWee4yTb4vrgLeUVcBu2duQx8Rcv4pUF60Mtb9k4jEC4T7rXZpUmOrXnGL9/eQnOuPPN4LKuY4DwSYzMbMmQC4mq8jPMWtvz6NYJLfjJAC4AcCwRzTb/Tk6rsTR88G5EJ1ZR2PYLHeqD90x0CrHgA+qLmybAbl24ffB+HdNPzCARHnh7BdY2tWDaR9HWqHQrvyv/Nhs3PbMAcz8LtsqcdTj/R0EuU/Eumg5Hpk97zbByz4SF6nkGWRVdZEGLwXvTIYT0SfEmElBmxaYd2Li9Fb98en6oi2ZLQJqHna3Gea2vzSpE0RTup9AHrlkuzwX3pDfPj7d+eyLAWC6ajhxqa9LRX6kpeGZ+g5mJmccx8wTz7+m02lOdybrPkF6Ysv/gWG0IpaSSWtZ+Q4T64FmenKw9l5feWIGTKWIqeHvnF6Fuworxk9/Peg10IZkPjdqs93oF+2+d34WFF/QK/b/PLpDGWKuuFmTHN4om4Bjhorn4z+/iq394S1pGnFuCf101Ge9tmsszHpixQhqap9oFpBa82OfaGXaaChOdFMMkQxW8vwW/o82w4OtrM+Fph5kjr6HAYKufuu9Bz2QxmwUvmy3skMXnuuQ5ery9KlWTqkDVgD9qdAOuP31srDZE4qF2hQ5jLxLmgwfcS/wxnp27BqOvfcaKXHHWHeSDj+eisbt9MpYFb1omPnX6iREkn3hoyJSWVa9CW4XIBTmfbtqJP7y61BFjXcxArTwO3pvfxY54aL6ycANmrdgiLWOfAVo4b+R8E5P8yKc/WoPr/jkXv33Ou7ZwMemC/d5ywmq0xhICygjlv2ZbCx54e0VgfUGJ2sQAbH1tVkmusDdoQcFFUzCU2nJ5x7WXzRq256uKY8ED0WfMqlI1Cj6K5dCrrrjwfxUL3n4xQ1evkXSa580ZnbKUx0EdRdVF8/bSTWi8eho2bjcmWDj8+q5B1nafB5SfFCqDwDUyCz6gYrdLR7w9yC77huZWT0ig41j7Detbyon0xg05WMVydPjgbS6a37+02Nret3ut5zgxCLlpR5vSRCc5krdDK/dScJ3n3P02Gq+ehtlmJJVSqgKzju2tHfirLa++jC07/C148Xu71WTCwyTzagaWIZ/5HwVDqaU97/jt7qrsozB55sCxgyDXWV3IQHpcqkbBq85k3buhJ3p2857M8w7dQ+n4Ha0dSkrUEUUT6oN3Kvl/vL/K8sn/t2SSS1CfVnXR/PG1pQCA372wCC3tOeyyzRYszGQN88HLtwe+YZh11WYJH63ahuUbd6CtI48PVwXn7ndPKrPHHrsJusns8q3d1oLVW4MXP3Ef45ABwTpeRdGKc+z+Hau2FOQKmndhnE+WbAtHJl7hULcrwlluxjIjkuXqvxv58YMW/JDXGExza/A1BORhqh+t2uYqw8pRXVY6bC5cl5a2nFPBe1w0BRlyefbEyNsJMnxkLsskqJpkYyr6/ckrJmPc8L5Sa/+XZxyImcs3h6Z4/Y+H3sdXDxke2lYUC95d5vl563D2xBG+ZZNw0YgZsQ+98ynac3lceHijtc8dRePrg/dpKuj3iodjNpPBl8z8GxcePhJT316BPQf1DJX7ydmfYVCvbo7UCG7C+oI49rBfvRTaniDOTFaVwU6nBW/7bCvTJhmsFcro33NW47xJTuNEeZBVss2vb/ltL2QeVbHglcQCAOxqCzdUlm3Yjv137+PY9iVXTpc8s/KYxB2vGEYPM0NEuO5szznkdp9a5sJvD08p4X8CVFIixKFqLHgVF834Ef2sct8/brRn/yPfOgwXHT4ysI7XFm1QHGQtfP7HB5/hvHtmBJZ1d5ygqCAVBRqGPb/H0g07HC6adpfl7u+Dl28PdtEY++xKSCwIscl0FwWFHn7/0dk4/953Aq1Bv5BZP/+yCnJrN9iGl1mObR15nHTbdExfbMxOtc4VwdfXG6Y4Xl7gzA9fzEQzy00hUWQyxFBK0mu8BK6nbHLP9E+s9BZ+GNa4//l4dq43mstuwefy7JBFNt4hNoW9OQUpeD3IGkLUZGNjh3nTww7s1Q3Xnz4WN5x+gO9x44f3Vfbp2Xlr6SbffWJ92GH9ugMAGgf2kC5BaJUP6EeqCl5EIgBGZ7a7aMTnggUfzQevIp9dTjHOYD1QFH5CkO81dNHqGIOtUgse0S34Ndt2Yf6aJlz7hDFgL5QPueqyHxk2t+HTzU4lp+yS8H1oSRS8z9XOeiz4ZFwNKgoeANY3B8+nMCx4//Nx+YPve7Yx2PFQsC9Y7lbSd7221Do3Yfde0O5OOZO1lEQNg48bNj+4Tz3aEphMZEe86tdkCcePGYL62mxg2KdKlEoYIpYYMBT5ywvWW99FxspwH7z479wfOAhsKXFvGWvCSIQHaNBSim7sg2hRkafbDU6BFmTQiSMdmREdKZcLZWXjKnZx7P56oLg4+Lx1jtyWqrwO8aYZNRdNGKrjCOGrqanXJWB2Rso5Fbyz7O0vL7Hul1AFr6No4hN1JqsoPqxfd/z0VGcWY7eC+NL4odbnnW0dSi4aGX433rqmVsxZtRUZItRlM+jIc+ouGnuuj08378T9by23vgvrqbBilY/ChDMOWGwLytctHhqyqei7rHbD5bcGtqRKKuSGVrzfZy7fjEfe/dS3zrBqNjS34qZnFji22QchP/h0C/7+vrEUI5G/eyTsmm51xYyr2h8//OtsvP/pFjRePQ2rtuzExu2tmPbRaqkMj5rnwY2479x98lfPzMfvXlikJkgR2B9EMqvfbcF//d53rHL+KTicE9B2BFjwdsLGv4IevP171AUeG5cqGmSNZzuMHtIL3zxyT8c2t0Lbyzb4t7MtF8tFAxjRHf17ei/ki/MNH+peDT1RkzVioIN+T5BBEsdFs8uVb1v4538xbT4uPWqvwCiaPa95GpNHDXTJ57RE7QEC4tze+PR8X9n8LJ1XbG8Z4sYWN83Ctc3IM2P/3ft4rl97Lu+IllFNPXzmH98GAJw7aQ/5OedgF82zH691fF+yvhlTbnnd+n6GLeKCQNbDUDw4BTIf/KPvFRRuU4tTwata8Ku3teCRd4x63lyyEfe/tQLzzWRa7hp+MU1+vdwuGnGcbB3WtNnvumc929il4N9YshEfr96GQ0YO8J3Zmmd23EfbbW+7wRFiYS4a/2NPOnC3wGPjUkUKPlr57rXGT5c9Od0X0f76tLM1Fzvfy41Pz0evbv6nPEOEmkwG23a1e/yqdoIXBFHzXTonYrmsbFdHDYuieXOJc3zBuUiKkUP/9pcWo3ttVuk6+bloLr7/PU/boq0TbjUU5/KbTvHcSL94ah7+8vYKdDfDDWO44KU358X3vxc4XuPmr+8VYr+ZgZ51WeywPVzZ2scuH7z3fNhTNLjfhqJkUBSLuTMDi9bZ1zpVO74wyCp898m6L8MIay7P3msnjvHN6cROV6fDgg/Q4W0dwcIEuWh6Fjk3x4+qcdHYB1kP22sArjp+n8Dyh+01ANefdgCul9ygR45ypi22hzDtbC/Ewc+45jhM2X+IsoyPz1rlcIW4yRKhNkvYurMdL8xb51suSPnfM/0TZXn8EJ1bjOz7KVy/fOLuFXfWbmvBLS8swo1Pz1da8UfJRSPKylw0ruOnL9kIoHBDh6mg3vXem81PaQYNnrvp7opnH9DLaVw4xghszUU1KKL6nAGvIrSPEdzxyhLf4ywXjSl8UuvxqhK+Ehd7DBZxhJ8xxDCMHGHYbbc9QO3pw92EWfBBb1bZlOLgq0bBu0fvw7oZEeGiIxrRp947S3DM0D5YftMp1nf7CPfO1hy27mxHfW0Gu/Wtx0VHjCxKbqdMaqPpl/xlJp760PCV/t20wNKgLZdHa0fO17con77vdtEwdtrcQSqetHyerZDJ8LLebZ4bzSVnkBI6bfxQ6VuW370bJSTR/ibI7HQrEtncTgxs3VWYqh91ZShVF40db3Ix4/+zc9fiN88t9D1OGFaqC9AnTWuINZDP+18jPwteZJPsbfaDoOytdsJ88EEWvGourahUjYK3k7QRYb8xd7R1YMHaJuy7mzHBwm2VFUOGSDqFX8ZsM7TwqsfmKNdfH5J2Vsb2lg7fG8RvsNl+z7HrdVfl2nTk2fJ/+2HP/+EmzIoM2l2TJaxvbsXSDc4Jb1EnAMnimt0Pb/eVFqd51eadDrdX1ARycfLxuy+xuOZhLr/CIKvxfcn67bjyb941VtMiLOBBlmxMnB6/35ZnY5C1V300BR82/hVowaeUDbcqFTyQrJKvsZ38lvY8ZizbjAOGGgo+yfCmTEY9HjbOA//9647HnecfHO2YT7f6vvLv9L1BCuXvmb7M0fGVZvUy4xNz1Sg/RC3um6a1I4eprkRW7haDJKjLZpDLM467+bVCefaf7u73e2Q53N0D5/a3Tnsc/CebnL89qgUfZ6KTX6hrWE3i1rBf86DF6JMm7OHH7O0joj/6KfiOfB4debbe5NyLfvsR5koLuixawUeAoR4poYIsouV8M3eNXcFf6orGiQqBHA+TwLJEWLEpWAm66VFXE1j/T07Z37PtW1Nn+iqMnT7LqtlvqFteWOS4CVUikJRcDK5BVsE9ry/Dk7NXO7Z5HhaBk6S858fIeeKj4H2qkuWPcVuC9paIyJELxU7UHP9xMoq621TNwOgXJlkqXg1Zr1ZmwYsHZovPilut7Xm05xg9hYumzX91KTvFRNFoF01EkrTg3Zltxw7rgwOGGjNh7a/ixT6F88yoUbXgARz9m1cjtxH0hnCgZHYv4D9oZw8fs+O2du0WaNS8PH74DbL63bSyY2XILmF7Lu878Os3AC1zh3kUvLstH8FKYcG7Y+nFNQ+LirEmOqUwuHrx5EZ8c7LTaPK7x/wMF9lMVhEe6Q4Ptu/vyOUtH7yqBV9MLpq0FiyqTgXP8WYr+iGsFGGt2/3udgu+eAUP1KU0mi4IkrG2JmMpJpHEqU99ja8S81sY2W2B2xWbSoSHkoIXURuusj0kmUK9x/rvkxlSHXn/PN9+N3U3SfpXt0LJuFw0fm1EjYqJM0/jdy86JyWp1mG5aFKw4Af3rsdXDh7m2OZn6X7xAHk0m2wmq3ij3LxDnnO+rcNw0fToVgOiCIOsIecgl2cc89tX8fN/faxUXxJUpYLnsBkoERE3oujM3W0xq/YbWdW94gdz8AxWO3HD0YIGceuyGcu1MKxfd0xqHIAxQ70ThwTbfRS825qzPwhUffBhiBJu60olaZN4OMiiZWTuuFyOfRWY32QZWXZAt8/X0RQl12WTWJdXuHnCanru43VYvK45FRdN/x613ush6b7ZDPmG3xoTnZzXSAzM+q1d29qRQ1tHHrUZQreajPLckrAF78XYUlCodNJUzUQnN6K7De/fHau27MJZE8NT/PohOtmhew7Ebn3q8Z1jR1n77BZ8sa9ZeWa0KL4ObvKxPsIIWkmpJkuor8kCaEeGYM2q9bt5fRW8q5/b44iTs+CN/1c9NseRvllloetH3l2Jnt1qUF+bdfyGsyYOl6qJtlze96Zs9XEJuV0egPdh5FZKSanIuOvy2onyFvDygvWpuGj696zzvHHK3pgyBN8kOB15b7pgYcG7I6UErR15dOTzqDUNnsQGWcswTlFVFvwPpxiTm9hmwJ81cQSW33QKfn3m+Nj1CiOiW00G/3vmOIwY0MPaZ7fUih0oyeXZMbMxiA3NanHiboJcNDWZDLrXZa1y2QyhPce+HVes5mNHtvCwXYmqKB+/G8+OfRDd/qrtdhvJfOEL1zXjR49/6JHzxyfuJx1kfXH+OmuFLTctPgtrfyZZSORfcwqDv8zscQfF0ZGXSAb2o/rsZcxYthkrN+9UeurkJZEqSdC/R11gVlVBLs++0+eEsrYjzo+f4jZ88IyarGHBqyv4+Nkk06KqFPzhexdyolhLuiVYv0w52l0CQSvvqMCsPqDT5Fq1SGWxDCB45Zi6bMZ6YNVmM6jNZjB75Va8+4n6bE0AuPt1Zx4Su4K3v+6ecdAwTP/xMZ7jVcLs7MrwzD8Wcrrc+epSR7nagDcW95tClkjqotkSsD6onwWvgv1hUpMhzAlZ1UqGLEw37sLrbs65e4aSZW6smpRIkw72GNBDKcdUnoGv+SyQ89Sc1fiDq0/8beZK7GjtwDafhb1b23NozxUseGUXTZgPvsSzfIEqU/CiL7BkWzEErVZjd8ucPmGot0DEdlRDsppc+UeOGj0I/+/ovUKPC7Tgs2Q9pPr3qLUejn+bWdxsWbuLxh7lks2Q420oCvZ7ZdkG/3DRoDEHt9WZyZB0bCNIie9UvF4y7Jdiy85234W5g5CNOdgtyWIG/pt2tSu9cTH7j1EIhvTpFlrPFcfsbX1ecMOJ2K1vvUd+96Cr4Oh9Gqy5KXYem7XKs3D9x6ubcMDPnvP1wa/e1oKmlg7UZAyXpV+0jZuwc5WE6ywqVaXg680BzwE96xINkzxkZH8AwHmTRgaWG9ynvqh28qw+carZlUEwl+dAa1UQ5oMXFn7/nnWJ5RWxW/D2iASVdSjdS7JFJSjs1G1RZTOEVonLJcgidrvUdutTj5u/Fu4OZCRjfMj6i92llqH47RCpDdjKEnq5kUUVedqDkS77B1NGW4aG24K/MiDHVNIGck3WcFm68+37ETb/wP6gKFVSttQUPBH9iYjWE9HctNpwM3ZYH9x4xlj89szxlhWfxAozu/ftjuU3nYIjRw8KLfvkFZNjt5NnxrUnj8E3jmgMLdu0y51BUM1aC4uiEYOU/XvUJTboZ1fwdv+wirwyhQvIfdxu7jz/4MCoGpmLRqbQlqz3HxNw1zGxsT++esjw2Ep17DD5A+2O8+QzkGW/z36OCWTFc0eFiJSszo5cPtT9oHI+arMZLLrxJPxgSkGJuwMXenerlbr1gGRDow15CLv1qY8QJmmcq+P2GyzdX8roGUGaFvz9AE5MsX4PRITzDx2Jvj1qE3+aqzJ+RD/cfu5BSmX7dncmOquryaBvj1p8T7JerBu3VZnPs5JFHBTKWZPNWFaGYcGHVqfklvKLtgl6mxAUM2A4fkS/wAeaO+Imk5G/Rgdl9nQjLOrQNUdYPujWr7t84YeRA+WurDAfPIPRW5JQTwWi8FwvgPEWE+ai2eEzKc6O7LfYZ9QeP2YI+vao9XXrJT3QW5PJYFj/7p7tfu4mkS5YxXAplX5KTcEz8+sANqdVf2j7YpA13XlDUk6zrQC17Jcn42dfGoOTxnoT+rvjZoWLKc4K63lmZCUKc5ArJa298/36q+Pw4xP3tb7XZAiTRxlvKX3qaxyvkX4PjwsOC3ZbAU4fvB2VB1IxCj5L6qkfRPk4qXbtqF67zTvasL3VO8jnpxz8XHeymcn2c8bsTX98xN4D3YdIySha8I/PWoWXbIuxyFAZq5D1h3ZbBMxhexXkvu2cCZ6yYW8RN54xNlQGOzVZstZJtjOkTz1uPdvbvrDgVRIGlsr+LLsPnoguI6KZRDRzw4bgvBKRMM+gSv5xP1RvhEe+dRhe+OHnpfsyGcLFk/fE4N7ep747WZdIUBVHweeYfZSZc5tdIZz1uREYN6yfY9+1p+yPP5x/MI4a3eDwwctcAQ9cMsnzFiLDz4KXPZDcFDMwlc1QpMWMsxkKnawShszXfOf5B+Olq452bGvL5bFys9fN1EdyPmuz/r9DZczGreDPPERtTkiGoLT+8LZd8mgUOyrRYbLfuOfAnjjjoGE4ctQgnDdpD2u7METshFnwZ/tE2vhRl81giGRcLWtOgHIjfPAq/brT++BVYea7mXkiM09saGgIP0C1XvN/MRb8A5ccioW/CPcyHb73QIwe0juwTDdJCKX7GgsLXjUfjZ18npUsB7eFaD9GKMSTDtwd2Qw5cqDLFMlRoxuUQkNl668C4Rb8oF7d8OeLJ2G3kMHrgZJlEAHj97gjKIIgHx98FI7ex9uHTz5wd+zd0Evp+J9/aQz+68T9HNtqs5kACz74HDK84bvq90TxDzzBBYeNREPvboEJ+WT9N5Mh/O7sCXjw0kOtORqAfM5JYL71DCnfVw2mMVaTJQzs5e1bNRmSnsNpH62x9ofRZSz4tBg33Eictd9uwYo3CONJnUy+d5Up9LIUs6rkWN6x3JaCu0yQgrBPJvIbrFaR2W9AVPjg/cYsXrrqaEwY0Q/XnLyfdL+gf886HLRHP8/2OCGCxbwx9OtRi2N8BthUGdirG779hb0d22qzGd/+E/a2l8uzp4wstvzQPQd4tmUo+Hxcf9oB+H+fDw/NBYB9hvTCe9dOwTUn74//O09+vVWiwASyFZCColiiTEKsNftNTTbjcXECxvkLCt6wP4gAuQHydoSVwIqhahX8qeOG4vUfHYMv7FvcDZcU7osuoz7gYfK7s8fjwsP9/d1+mSiFmyVr67R2gl4n7W+8fomZ+nWvsyyeqAhR7GMWD1wyyfrcU+GcAUb4mez1Po6Cj5NqVyBb/OV7trQWcanNkq+CVxmoVnHjyDKJEgWnfhjQsw4jB6pNsBMKMZshnDpOPjCvutgNIFfYoq//h+sBabSvmZKg3QAAFM9JREFUXLX18KjNEAb18vbt604dE+j4dSv0g/bo57kGF/7pXXWBiiDNMMlHALwNYF8iWkVEl6TVlh97+EQelIOgxbYFQSsunTZ+GC6e7P96y8wOhXaCmV1PKPRXrvqC8d3toglSgi5dN+snUzDrJ1Mc2+pqMnjv2imewaggy7KHqbjtMeR7mTNxRw8uvHEJ2cNmi362dZf0TUTFahs/wmn5F5M0S+YhOPfQgt/4Ryfs6y2gQJCLRuUh5n44yKzP7xw7Cke6/Nrrmlrx9/f9J7kx1AbKjTbDy0R5IMvKimsnG++KUrd4k6jJZqwlPYf2LbgJxw7rGzjDdoBLwQ/s2Q0v/vBon9LpkmYUzbnMvDsz1zLzcGa+L622OgM9FKzRI/b2j7PPZkiaq1zwsy8d4LjZbj5rAt74r2M8itbd0YMGId0TnQb26oaBEosGgMeKZwB/vewwadkLDhuJrx48HKccuLu17cnvTMb0Hx/juTkA/3wvdtZsa/FsU3njH+eyXNtjpNoVyCaG2S3sK44ZFXgN/ajJUnEK3uOi8Zbp16MOv5NEhoSNSfjJ5XZtqKQciDog7kacf9l4l0r77rprs4RMhvDqf34B933jc876AkR19+HuddmyGZtV66KpNHqGWPBvXn0svhaS8dKvk55/6B4Y0qfe4W6pr8lgeP8ellUpDnVb7EEKIspM1rsvPARjbLNOmRmfa/T6dQHjtf/ms8ZjrE259q434ptlCkNlqvim7U4X0sCedUo+XbePuRgfvMz4d1u4qhPv/v2dI211ZCIvdGFHdXZ01HTXzOyrlE90hQWrVB3JgpecRxHiKnt7jOKiEb9JPJwbB/X0RCIFPTAG9nQaOz0V1ihIC63gS4Tbgr/u1DH4+7ePsL4P69c99OYPS0dsv0FFXUP7Ga+W4iZ3txH0ih3FWzG4dz3O/lwhDI3ZX96omTD9ZhLa3UJu5TTruuOV0je7LdQoPvhJjQNw01cOxMWTG80t3mPDFtr2w55XJWiAXs1Fo6ZgovjABX4K/vrTxjpm5ao82FTdPYC8b+UsBe/9vVEeHoMlE5ncYx0y37zAHTrco875cEhr/VUZWsGnxFPfPRK//uo467v7on95wlArx40qYf3CbrmIsn/8+iG47ZwJjnje3t1qrAlOQaFjxcTqBln/65u97pQg+veQh0HafdoqeeBluFPJui14v5vxt18bj3sumohzJu2BH5r5UfbbzZtmwK00ZZbf6ROG4tazJ+CuCw6xttmLuaNO7ApfRSl7XTTyY3rX1+Jf35mM3fvKw1IbTTeDmMnJLFfK3ziiEdkM4dHLDsck8y1ORaWpxI/bOcqVOiQfYMH7/WZxeRttLpQR/Y3Pa7YVor/c/cDvHMnKuo27Uq5fW7ULfpSbscP6OlwQE0b0w1XH74ObXzCWRhM33QkHDLE6lJ2jRg/CjtYOXHvKGGuiUNigod1SEBbTwF7dcPoEZwa+j64/wfoc9FouuuGFh4/EJEkYnRu7RR3Uh4MGiwHg5auOdhz/9cNGoq4mg2v+8ZGjnD2+u7Ujh6NGD8L0xRsdZb52yHA8Nst/oNBtsbtnsvaoy0rj+O2ThfrU1+Kvlx2G/SXZDD1uItfpPmRkf9x2jjds0G7xjhrc27Wv8FlFKboVfFAvGje8n+8A+TUn748sEZ76cDX+OXs1GBz4dtGrWw2G9++Od5cr+uAjWrZTvzkJe17ztPVdXDuZS8qv/RvPOBDX/OMjHDC0L5Zv2gkAOG3CUDwwYwXGDS8MwLvvE9lYkdWWq/medeVTs9qCLxFEhO8eNxqTRxmzY8Wr7V0XTMRPTh3jKf/AJYfiH/8xGYeM7G9NnnG/5oobUUToxPH1BSl4YYWfechw39A2O1PGDMGd58uTYglOGz8UJ9sGV2Xs1dALowYXJgZlM4RzJ+2Bxy8/3JrfADgto/Yc454LJ3rq+k1IZke3Be9W+O6z84V9G6QPu0P3GmhFXNhxuxLEV/H2EWcikUPBm1/69fCfUexe53evkElXfrH8PetqMGXMEKsfMhvr+Apk+XJEH1IxzqO6Ltz3Q85lwdvfLmRVnzpud+vN2m5Vf65xABbfeJLjOruvIxHhc42FN3B7eK/bEFMJkU4LreBLzF0XTMSTV0yOtTiI/Qa4/rQD8JWDDStyN/N10e3rUyHIRSPi8qPceO7XZjvLbzpFORGbjImNAxznzX3jxDmnboXujv5w+//vv3gS/vb/Do/cjkCkzhAT8OLkviEQpuxvKGGhSxoCfMJun/S+u/W2JoZdcNhI3Ot6MF578v7SeqxxHPO74aIxtu3Wp97Kgmp37Ymfp5IyRBb9EgUxk1XIOaJ/D2uNBNmEvNpsxrLs3bNg3WMLMkPoscsLY2hHjW6wHhZE5Ag4ODiiKzZJtIumxPTqVuOJvVZF9LG6bAYXHdGIb02dCQDWVP6kLfjbzz0Ij7630tFZAeCfV0xGq8/AZ7GrWoVhzzeSRPZAd+6Xey6ciNcXb8R/PjYHQPJZ/4b3747F67db5ymKP/aWs8bjyr/NQYYMOfMMvLXUcEkN6tUNi33SGp81cQReXrAeby8rzJ4UD4TJowZiypghjvJ+D33rQW/+c8fBW4rfdkzQYjmCMw8Zjj0H9cT44d7JVlEQ18rK6AngcyMH4C4swx5mBsq9G3piqblATI25LCUQ3peUUnFnCpO5Hv/24di2qx279y24LY/bb7AjKdu1J++PQ/caEMswU0Vb8J0I8UoqXnfPNyfRiFfJeBa8f8cd2q87rjx+H8+r8IQR/XDoXvJEbKKTu6fbP/XdI2XFI2MfvN1hy1D4x6/7u4bev+546/N9F010yHL96Qc4yg7uU48zDxlelIUNAAdLUicAwEOXHorfn3uQZe1FqV8oYjKnymczhENG9sekPQfg+tMPwEtXHS3Ncti3Ry0evPRQn1qDFde9F07EcDNlrrDMhTXO7PTB2103AvExyAc/vH93XHHMqETWbgCcg9DiDWy86U9/4orJlpuvtiZjzaYOy0SpouBFmQwZ96JduQPAfd/4HEYMsEV+ZQnjhvdzuCOTRlvwnQjRx4SP7wv7Dsbym06x9qtO7bcTJf+HCkTkkEkwVjIVPg52j8qRoxrwjSMa8Z1jRwWGrdkH3Y7bf4gjda3Mbw4Avzt7Ak66bXpsOR/+1mHSwdnBferxpfFDschcLi6KBe+e0wAYisTuMvJTQ24FNWXMEDw/bx1GDwlWLlPGDMFdry/Fqi27PLIaFrzXB7+PLf/T0fs0YNqHa7BvQE6oYgYhX/3PL1gPj/MP3QMPvfOp5S8nACeN3Q03nH4Azv6cYQz1qa/FqMG98OGqbajNFNbgDbsOKrOiaywFHxC4YGsmysSuuGgF34kQ44G+k15idBiVWPFiuOCwkZ6BzGKwv0rX1WTw89OcFvjIgT2wcvNOxzb3zRmU80egkgY5iPrabKC7SliZURR8r241mDCiH753nH9+G1U/9lkTR+CUA3cPnYAHFPqbkNU6nbZBVgbjC/sOxpM2CxkwophOGLMb+gYMAhczCNloW2z+htPH4uenHYBPbde/JpvBBYc3Oo6xLyxvuWhCLHiV+0TksAmq6/6LP4cpt7wOQC0BYbFoBd+J6FVv3ODfD1nxSZb0qlzc8OVoiyyEEXYjvnSlN+eH+yVF3KyNAdPH3TMXk0a4xqJE0WQzhH+GLAnZK4I1HKTcH7hkEhatM3z6/3P6WNzw1DxrsLCg39nz8HSPLxGRVLl/fp8GvL5ogylHMv01kyFkFAZzheXcoy5ryR83Nv2lq4628uGLyVBBbrdRg3tjUK9u2Li9NdbEsqhoBd+JULnBn/iPI6yommok7EaUvcWIG++MgwrzAf79nSOtWb4yenWrwSnjdsdXDhqGS/4yM6a0/gglU+wKUm76dPe/pR++9FDMXb1NqZ6jRjfgqNFGeO4+Q3rjgUsKPvwfHr8P1je34uQDd1daik/G1G9OwremzsQL89ahe206asjvzIo+1KNbjfWwj/uSac/z737T8UMMTGsXjSYyB+1RvpCsUiAMeHuuljCyGcLMn0xxuF0OtLkR7rtoosdfTkS447yDU5t1qKIM/u+8g5QXChEEDVQeMWoQjpCshBSVof264y/fNOK+hZK6/Ghvit4wRI4hlUR8SSLOec+6bOE6KIZLDZes0Sr47rGj8P1HZ2OoZJk/O4VkZlrBazQOxI0YdXGUoEHY4/Yf4rtP3IznTtoDVxwTXYn50a97LQb0rMNPv+Sd5CZQmVxWbuprs9JBdRXEYHdaybj8HnUi/1C3mqzyICtgRIIFKe/TJwzzzBqXISYmRsm9Exet4DU4br/B+LxkqblKRPhMo6R/LZa4CiyImmzGEb6ZJN84ohEbmlutJeQqlZ2WBV9aNZSzLY7d13RpDe/fHfddNDFw7dikIsEaB/bEgrXNvmsVJ4lW8BpPrutK5o8XHIJH3v0UezeorSTUFRGRRdOunlZmSYL5zy/uiyv/NhuNiqtCJUW7aa3XZDMYNbg37rrgEEweNQi9utVAPrsjWSaPGohnP14butxiEmgFr+lU7DmoJ/7bZyq9pnMxZcwQfPjzE8ILJkzOdNGI5GYnHLBbUPHE+fphI7H34F443GeyYJJoBa/RaLoUYl5GKfOy2yGiwNXbkkSnKtBoNF0KkaTvgIR86pWMtuA1Gk2X4uQDd09l4LwS0Ra8RqPRVCnagtdoNFXF8P7dsXdDT/zsSweEF65ytILXaKqUh791KNY1RVv/throVpPFS1d9odxiVASpKngiOhHAbQCyAO5l5pvSbE+j0RQoVaSGpnJJzQdPRFkAdwA4CcAYAOcSkf+8bI1Go9EkSpqDrJMALGHmZczcBuBRAKen2J5Go9FobKSp4IcBWGn7vsrc5oCILiOimUQ0c8OGDSmKo9FoNF2LNBW8bJqYJ2UbM9/NzBOZeWJDQ+dIeKXRaDSdgTQV/CoAI2zfhwNYnWJ7Go1Go7GRpoJ/D8BoItqTiOoAnAPgXym2p9FoNBobqYVJMnMHEX0HwHMwwiT/xMwfp9WeRqPRaJykGgfPzE8DeDrNNjQajUYjh1hxLcJSQEQbAKyIefggABsTFCdpKl0+QMuYBJUuH6BlTIJKkm8kM0sjVCpKwRcDEc1k5onllsOPSpcP0DImQaXLB2gZk6DS5RPobJIajUZTpWgFr9FoNFVKNSn4u8stQAiVLh+gZUyCSpcP0DImQaXLB6CKfPAajUajcVJNFrxGo9FobGgFr9FoNFVKp1fwRHQiES0koiVEdHUZ5fgTEa0norm2bQOI6AUiWmz+72/bd40p80IiOqEE8o0goleIaD4RfUxE369AGeuJ6F0immPKeH2lyWi2mSWiD4joqQqVbzkRfUREs4loZoXK2I+IHieiBWafPLxSZCSifc1zJ/6aiOgHlSJfJJi50/7BSIGwFMBeAOoAzAEwpkyyfB7AwQDm2rb9GsDV5uerAfyv+XmMKWs3AHuavyGbsny7AzjY/NwbwCJTjkqSkQD0Mj/XAngHwGGVJKPZ7pUAHgbwVKVdZ7Pd5QAGubZVmox/AXCp+bkOQL9Kk9FsOwtgLYCRlShfqPzlFqDIk384gOds368BcE0Z5WmEU8EvBLC7+Xl3AAtlcsLI13N4iWV9EsDxlSojgB4A3gdwaCXJCCMr6ksAjrUp+IqRz2xHpuArRkYAfQB8AjPIoxJltLX1RQBvVqp8YX+d3UWjtKhIGRnCzGsAwPw/2NxeVrmJqBHAQTAs5IqS0XR/zAawHsALzFxpMt4K4McA8rZtlSQfYKy78DwRzSKiyypQxr0AbADwZ9PVdS8R9awwGQXnAHjE/FyJ8gXS2RW80qIiFUjZ5CaiXgD+DuAHzNwUVFSyLXUZmTnHzBNgWMqTiGhsQPGSykhEpwJYz8yzVA+RbCvFdZ7MzAfDWA/5CiL6fEDZcshYA8Od+QdmPgjADhguDz/Kch7NNOenAXgsrKhkW0Xooc6u4Ct9UZF1RLQ7AJj/15vbyyI3EdXCUO4PMfM/KlFGATNvBfAqgBMrSMbJAE4jouUw1hg+logerCD5AADMvNr8vx7AEzDWR64kGVcBWGW+nQHA4zAUfiXJCBgPyPeZeZ35vdLkC6WzK/hKX1TkXwAuMj9fBMPvLbafQ0TdiGhPAKMBvJumIEREAO4DMJ+Zb6lQGRuIqJ/5uTuAKQAWVIqMzHwNMw9n5kYYfe1lZv56pcgHAETUk4h6i88wfMhzK0lGZl4LYCUR7WtuOg7AvEqS0eRcFNwzQo5Kki+ccg8CJDAIcjKMiJClAK4toxyPAFgDoB3GE/0SAANhDMgtNv8PsJW/1pR5IYCTSiDfkTBeGz8EMNv8O7nCZBwH4ANTxrkAfmpurxgZbe1+AYVB1oqRD4Z/e47597G4JypJRrPNCQBmmtf6nwD6V5KMMAb5NwHoa9tWMfKp/ulUBRqNRlOldHYXjUaj0Wh80Apeo9FoqhSt4DUajaZK0Qpeo9FoqhSt4DUajaZK0Qpe0+UgopyZJXAOEb1PREeElO9HRP+hUO+rRFTxCzFrug5awWu6IruYeQIzj4eRKOpXIeX7AQhV8BpNpaEVvKar0wfAFsDI00NEL5lW/UdEdLpZ5iYAe5tW/2/Msj82y8whopts9X2NjJz2i4joqNL+FI3GSU25BdBoykB3M2NlPYy0r8ea21sAnMHMTUQ0CMAMIvoXjERYY9lIggYiOgnAlwEcysw7iWiAre4aZp5ERCcD+BmMdAsaTVnQCl7TFdllU9aHA5hqZq0kAL80sy/mYaR8HSI5fgqAPzPzTgBg5s22fSKJ2ywY6wNoNGVDK3hNl4aZ3zat9QYYuXkaABzCzO1m1sh6yWEE/3Swreb/HPT9pSkz2gev6dIQ0X4wlmXbBKAvjHzv7UR0DIxl2gCgGcYyh4LnAXyTiHqYddhdNBpNxaAtDE1XRPjgAcMav4iZc0T0EIB/k7FQ9WwYqYrBzJuI6E0yFlR/hpl/REQTAMwkojYATwP47zL8Do0mEJ1NUqPRaKoU7aLRaDSaKkUreI1Go6lStILXaDSaKkUreI1Go6lStILXaDSaKkUreI1Go6lStILXaDSaKuX/A2SjEORO+zdTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss through epochs\n",
    "plt.plot(loss_history)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the model and print out a few images so we can visually see how good a job the model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.load_state_dict(torch.load(MODELS_PATH+'Epoch N4.pkl'))\n",
    "\n",
    "# Switch to evaluate mode\n",
    "net.eval()\n",
    "\n",
    "test_losses = []\n",
    "# Show images\n",
    "for idx, test_batch in enumerate(test_loader):\n",
    "     # Saves images\n",
    "    data, _ = test_batch\n",
    "\n",
    "    # Saves secret images and secret covers\n",
    "    test_secret = data[:len(data)//2]\n",
    "    test_cover = data[len(data)//2:]\n",
    "\n",
    "    # Creates variable from secret and cover images\n",
    "    test_secret = Variable(test_secret, volatile=True)\n",
    "    test_cover = Variable(test_cover, volatile=True)\n",
    "\n",
    "    # Compute output\n",
    "    test_hidden, test_output = net(test_secret, test_cover)\n",
    "    \n",
    "    # Calculate loss\n",
    "    test_loss, loss_cover, loss_secret = customized_loss(test_output, test_hidden, test_secret, test_cover, beta)\n",
    "    \n",
    "#     diff_S, diff_C = np.abs(np.array(test_output.data[0]) - np.array(test_secret.data[0])), np.abs(np.array(test_hidden.data[0]) - np.array(test_cover.data[0]))\n",
    "    \n",
    "#     print (diff_S, diff_C)\n",
    "    \n",
    "    if idx in [1,2,3,4]:\n",
    "        print ('Total loss: {:.2f} \\nLoss on secret: {:.2f} \\nLoss on cover: {:.2f}'.format(test_loss.data, loss_secret.data, loss_cover.data))\n",
    "\n",
    "        # Creates img tensor\n",
    "        imgs = [test_secret.data, test_output.data, test_cover.data, test_hidden.data]\n",
    "        imgs_tsor = torch.cat(imgs, 0)\n",
    "\n",
    "        # Prints Images\n",
    "        imshow(utils.make_grid(imgs_tsor), idx+1, learning_rate=learning_rate, beta=beta)\n",
    "        \n",
    "    test_losses.append(test_loss.data)\n",
    "        \n",
    "mean_test_loss = np.mean(test_losses)\n",
    "\n",
    "print ('Average loss on test set: {:.2f}'.format(mean_test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
